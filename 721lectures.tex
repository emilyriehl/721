% !TEX program = lualatex
% The following code introduces a new \if macro which we use to switch
% compilation between the published and internet versions of the book
%
\newif\ifmine
%
% The default is false, which means we compile the internet version.
% Un-comment the next line to compile the published version:
%
%\minetrue
%
\documentclass{amsart}
\usepackage[hidelinks]{hyperref}  
\usepackage{tensor}    
\usepackage{comment} 
\usepackage{enumitem}      
\usepackage{moreenum}
\usepackage{graphicx}   
\usepackage{ifthen}  
\usepackage{stmaryrd}
\usepackage[svgnames]{xcolor}  
 \usepackage{fullpage}
 \hypersetup{  
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
} 
\usepackage{mathpartir}%I think for \inferrule
 
% Font packages  
\usepackage[no-math]{fontspec} 
\usepackage{realscripts}

% Unicode mathematics fonts
\usepackage{unicode-math}
\setmathfont{Asana Math}[Alternate = 2]

% Font imports, for some reason this has to be after 
% the unicode-math stuff. 

\setmainfont{CormorantGaramond}[
Numbers = Lining,  
Ligatures = NoCommon,
Kerning = On,
UprightFont = *-Medium,
ItalicFont = *-MediumItalic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

\setsansfont{texgyreheros}[
Scale=0.9129,
Ligatures = NoCommon,
UprightFont = *-Regular, 
ItalicFont = *-Italic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

\setmonofont{SourceCodePro}[
Scale=0.8333,
UprightFont = *-Regular,
ItalicFont = *-MediumItalic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

% AMS Packages
\usepackage{amsmath}
\usepackage{amsxtra}
\usepackage{amsthm}

% We use TikZ for diagrams
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{makebox}%to try and fix the spacing in some diagrams with wildly divergent node sizes

\renewcommand{\theenumi}{\roman{enumi}} %roman numerals in enumerate

% Adjust list environments.

\setlist{}
\setenumerate{leftmargin=*,labelindent=0\parindent}
\setitemize{leftmargin=\parindent}%,labelindent=0.5\parindent}
%\setdescription{leftmargin=1em}

\newcommand{\todo}[1]
{ {\bfseries \color{blue} #1 }}


\newcommand{\lecture}[1]{\vspace{.1cm}\centerline{\fbox{\textbf{#1}}}\vspace{.1cm}}

\theoremstyle{theorem}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
\newtheorem*{fact}{Fact}
\newtheorem*{cor}{Corollary}
\newtheorem*{prop}{Proposition}

\theoremstyle{definition}
\newtheorem*{defn}{defn}
\newtheorem*{ntn}{Notation}
\newtheorem*{post}{Postulate}
\newtheorem*{ax}{Axiom}
\newtheorem*{ex}{ex}
\newtheorem*{nex}{non-ex}
\newtheorem*{exc}{Exercise}
\newtheorem*{exnex}{Example/Non-Example}
\newtheorem*{tf}{T/F}
\newtheorem*{q}{Q}
\newtheorem*{rQ}{rhetorialQ}
\newtheorem*{rev}{Review}


\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{war}{Warning}
\newtheorem*{dig}{Digression}

%\makeatletter
%\let\c@equation\c@thm
%\makeatother
%\numberwithin{equation}{section}

\newcommand{\cat}[1]{\textup{\textsf{#1}}}% for categories
\newcommand{\fun}[1]{\textup{#1}}%for functors

%commonly used categories
\newcommand{\Ch}{\cat{Ch}}
\newcommand{\Set}{\cat{Set}}
\newcommand{\sSet}{\cat{sSet}}
\newcommand{\Top}{\cat{Top}}

%math operators
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclareMathOperator{\cod}{\mathrm{cod}}
\DeclareMathOperator{\ob}{\mathrm{ob}}
\DeclareMathOperator{\mor}{\mathrm{mor}}
\DeclareMathOperator*{\colim}{\mathrm{colim}}
\newcommand{\hocolim}{\mathrm{hocolim}}
\newcommand{\wcolim}{\mathrm{wcolim}}
\newcommand{\holim}{\mathrm{holim}}


\newcommand{\op}{\mathrm{op}}
\newcommand{\co}{\mathrm{co}}
\newcommand{\Nat}{\mathrm{Nat}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Sym}{\mathrm{Sym}}

\newcommand{\coim}{\mathrm{coim}}
\newcommand{\To}{\Rightarrow}
\newcommand{\coker}{\mathrm{coker}}

\newcommand{\Map}{\mathord{\text{\normalfont{\textsf{Map}}}}}
\newcommand{\Fun}{\mathord{\text{\normalfont{\textsf{Fun}}}}}
\newcommand{\Hom}{\mathord{\text{\normalfont{\textsf{Hom}}}}}
\newcommand{\Ho}{\mathord{\text{\normalfont{\textsf{Ho}}}}}
\newcommand{\h}{\cat{h}}
\DeclareMathOperator{\Lan}{\fun{Lan}}
\DeclareMathOperator{\Ran}{\fun{Ran}}
\newcommand{\comma}{\!\downarrow\!}

%special blackboard bold characters
\newcommand{\bbefamily}{\fontencoding{U}\fontfamily{bbold}\selectfont}
\newcommand{\textbbe}[1]{{\bbefamily #1}}
\DeclareMathAlphabet{\mathbbe}{U}{bbold}{m}{n}


\def\DDelta{{\mbfDelta}}

%categories?
\newcommand{\cA}{\mathsf{A}}
\newcommand{\cB}{\mathsf{B}}
\newcommand{\cC}{\mathsf{C}}
\newcommand{\cD}{\mathsf{D}}
\newcommand{\cE}{\mathsf{E}}
\newcommand{\cF}{\mathsf{F}}
\newcommand{\cG}{\mathsf{G}}
\newcommand{\cI}{\mathsf{I}}
\newcommand{\cJ}{\mathsf{J}}
\newcommand{\cK}{\mathsf{K}}
\newcommand{\cL}{\mathsf{L}}
\newcommand{\cM}{\mathsf{M}}
\newcommand{\cN}{\mathsf{N}}
\newcommand{\cP}{\mathsf{P}}
\newcommand{\cS}{\mathsf{S}}
\newcommand{\cT}{\mathsf{T}}
\newcommand{\cV}{\mathsf{V}}

\newcommand{\0}{\mathbbe{0}}
\newcommand{\1}{\mathbbe{1}}
\newcommand{\2}{\mathbbe{2}}
\newcommand{\3}{\mathbbe{3}}
\newcommand{\4}{\mathbbe{4}}
\newcommand{\iso}{\mathbbe{I}}

%blackboard bold
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\kk}{\mathbbe{k}}

\newcommand{\sA}{\mathcal{A}}
\newcommand{\sB}{\mathcal{B}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sJ}{\mathcal{J}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\sR}{\mathcal{R}}



%type theory stuff
\newcommand{\univ}{{~\texttt{type}~}}
\newcommand{\judgment}{\mathcal{J}}
\newcommand{\term}[1]{{\textup{\texttt{#1}}}}
\newcommand{\type}[1]{{\textup{#1}}}

\newcommand{\comp}{\term{comp}}
\newcommand{\id}{\term{id}}

\newcommand{\bN}{{\mathbb{N}}}
\newcommand{\bT}{{\mathbb{T}}}
\newcommand{\suc}{\term{succ}_{\bN}}

\newcommand{\ind}{\term{ind}}
\newcommand{\inl}{\term{inl}}
\newcommand{\inr}{\term{inr}}
\newcommand{\pair}{\term{pair}}
\newcommand{\pr}{\term{pr}}

\newcommand{\bZ}{{\mathbb{Z}}}

\newcommand{\refl}{\term{refl}}
\newcommand{\pathind}{\term{path}\text{-}\term{ind}}
\newcommand{\concat}{\term{concat}}
\newcommand{\inv}{\term{inv}}
\newcommand{\assoc}{\term{assoc}}
\newcommand{\ap}{\term{ap}}
\newcommand{\apcoh}[1]{\term{ap}\text{-}\term{#1}}
\newcommand{\tr}{\term{tr}}
\newcommand{\apd}{\term{apd}}

\newcommand{\UU}{{\mathcal{U}}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\Id}{\textup{\text{Id}}}
\newcommand{\Eq}{\textup{\text{Eq}}}


\newcommand{\bool}{\type{bool}}
\newcommand{\true}{\term{true}}
\newcommand{\false}{\term{false}}

\newcommand{\is}[1]{\type{is-{#1}}}
\newcommand{\iscontr}{\type{is-contr}}
\newcommand{\isprop}{\type{is-prop}}
\newcommand{\isequiv}{\type{is-equiv}}

\newcommand{\fib}{\type{fib}}

\newcommand{\Prop}{\type{Prop}_{\UU}}
\renewcommand{\iff}{\leftrightarrow}

\newcommand{\mere}[1]{\|{#1}\|}
\newcommand{\im}[1]{\type{im}(#1)}
\newcommand{\ev}{\term{ev}}


\begin{document}

\title{Math 721: Homotopy Type Theory}
\author{Emily Riehl}
\date{Fall 2021}

%\begin{abstract}
%\end{abstract}

\address{Dept.~of Mathematics\\Johns Hopkins University\\3400 N Charles St\\Baltimore, MD 21218}
\email{eriehl@math.jhu.edu}

\maketitle

\setcounter{tocdepth}{1}
\tableofcontents


\ifmine
\subsection*{Index cards:} name, what I should call you, pronouns, year, relevant previous coursework, why you are here, something fun


\subsection*{Personal history:}
Prehistory of homotopy type theory dates to a 1998 paper of Martin Hofmann and Thomas Streicher called ``The groupoid interpretation of type theory,'' that I'll tell you about later. Key early developments were in the mid aughts with the major players coming together for the first time in 2010 at Carnegie Mellon. This expanded to a larger group at Oberwolfach in 2011 followed by a special year at IAS in 2012-2013, during which the HoTT book (not our book) was written.

I got my PhD in 2011. I started hearing a little about this in my final years of grad school but learned most of what I'll tell you about this semester during my postdoc and while here at Johns Hopkins. Point of this to say is that learning doesn't stop when you earn your PhD. Learning also doesn't necessarily precede teaching. My goal for this semester is to know a lot more \texttt{agda} by the end than I do right now. My promise to you is that I'll do all the homework before I assign it. I don't promise that I'll be able to answer very many of your questions about what is going on under the hood, but that's okay too: it's probably useful for all of us to be reminded that figures of authority don't always know what they're talking about.

\subsection*{Syllabus:}
The goal for this course is to change the way you think about doing mathematics. I certainly have. Along the way, I hope you all learn a lot, though as is typical in a graduate course, that's somewhat in proportion to the amount of work you put in. This also has a lot to do with your background. If you're coming from CS, you'll probably leave the course knowing way more \texttt{agda} than I do, but perhaps a bit less about synthetic homotopy theory. For others, it will be vice versa. Still others might engage most with the philosophical implications of these developments. I'm equally happy with all of these directions. 

There are two ``active learning'' activities. The first is writing your own formal proofs in the computer proof assistant \texttt{agda}. This is hard in the sense that if you make one typo the whole thing breaks but a lot of help is available. I'll say more about it when the time comes. In particular, I'm hoping many folks will attend group office hours on Thursday evenings from 5-6pm. In the first meeting there won't be a problem set due: instead the goal will be to get \texttt{agda} installed. Come see me if you're worried about technical issues.

The second active learning activity is a final project. I'm very flexible about the parameters. Essentially you should pick something that sounds fun to you.

Questions?
\fi

\part{Martin-L\"of's Dependent Type Theory}


\section*{August 30: Dependent Type Theory}

Martin-L\"{o}f's dependent type theory is a formal language for writing mathematics: both constructions of mathematical objects and proofs of mathematical propositions. As we shall discover, these two things are treated in parallel (in contrast to classical Set theory plus first-order logic, where the latter supplies the proof calculus and the former gives the language which you use to state things to prove).

\subsection*{Judgments and contexts}

I find it helpful to imagine I'm teaching a computer to do mathematics. It's also helpful to forget that you know other ways of doing mathematics.\footnote{Indeed, there are very deep theorems that describe how to interpret dependent type theory into classical set-based mathematics. You're welcome to investigate these for your final project but they are beyond the scope of this course.}

\begin{defn} There are four kinds of \textbf{judgments} in dependent type theory, which you can think of as the ``grammatically correct'' expressions:
\begin{enumerate}
\item $\Gamma \vdash A \univ$, meaning that $A$ is a well-formed type in \textbf{context} $\Gamma$ (more about this soon).
\item $\Gamma \vdash a : A$, meaning that $a$ is a well-formed term of type $A$ in context $\Gamma$.
\item $\Gamma \vdash A \doteq B \univ$, meaning that $A$ and $B$ are \textbf{judgmentally} or \textbf{definitionally} equal types in context $\Gamma$.
\item $\Gamma \vdash a \doteq b : A$, meaning that $a$ and $b$ are judgmentally equal terms of type $A$ in context $\Gamma$.
\end{enumerate}
These might be collectively abbreviated by $\Gamma \vdash \judgment$.
\end{defn}

The statement of a mathematical theorem, often begins with an expression like ``Let $n$ and $m$ be positive integers, with $n < m$, and let $\vec{v}_1,\ldots, \vec{v}_m$ be vectors in $\RR^n$. Then \ldots'' This statement of the hypotheses defines a \textbf{context}, a finite list of types and hypothetical terms (called \textbf{variables}\footnote{We're not going to say anything about proper syntax for variables and instead rely on instinct to recognize proper and improper usage.}) satisfying an inductive condition that that each type can be derived in the context of the previous types and terms using the inference rules of type theory.

\begin{defn} A \textbf{context} is a finite list of variable declarations:
\[ x : A_1, x_2 : A_2(x_1), \ldots, x_n : A_n(x_1,\ldots, x_{n-1})\]
satisfying the condition that for each $1\leq k \leq n$ we can derive the judgment
\[ x_1 : A_1, \ldots, x_{k-1} : A_{k-1}(x_1,\ldots, x_{k-2}) \vdash A_k(x_1,\ldots, x_{k-1}) \univ\]
using the inference rules of type theory.
\end{defn}

We'll introduce the inference rules shortly but the idea is that it needs to be possible to form the type $A_k(x_1,\ldots, x_{k-1})$ given terms $x_1, \ldots, x_{k-1}$ of the previously-formed types. 

\begin{ex} For example, there is a unique context of length zero: the empty context.
\end{ex}

\begin{ex} $n : \NN, m : \NN, p : n < m, \vec{v} : (\RR^n)^m$ is a context. Here $n : \NN, m : \NN \vdash n < m$ is a dependent type that corresponds to the relation $\{ n < m \mid n, m \in \NN\} \subset \NN \times \NN$ and the variable $p$ is a witness that $n < m$ is true (more about this later). 
\end{ex}

\subsection*{Type families}

Absolutely everything in dependent type theory is context dependent so we always assume we're working in a background context $\Gamma$. Let's focus on the primary two judgment forms.

\begin{defn} Given a type $A$ in context $\Gamma$ a \textbf{family} of types over $A$ in context $\Gamma$ is a type $B(x)$ in context $\Gamma, x : A$, as represented by the judgment:
\[ \Gamma, x : A \vdash B(x) \univ\]
We also say that $B(x)$ is a type \textbf{indexed} by $x : A$, in context $\Gamma$.
\end{defn}

\begin{ex}  $\RR^n$ is a type indexed by $n \in \NN$.
\end{ex}

\begin{defn} Consider a type family $B$ over $A$ in context $\Gamma$. A \textbf{section} of the family $B$ over $A$ in context $\Gamma$ is a term of type $B(x)$ in context $\Gamma, x : A$, as represented by the judgment:
\[ \Gamma, x : A \vdash b(x) : B(x) \]
We say that $b$ is a \textbf{section} of the family $B$ over $A$ in context $\Gamma$ or that $b(x)$ is a term of type $B(x)$ indexed by $x : A$ in context $\Gamma$.
\end{defn}

\begin{ex} $\vec{0}_n : \RR^n$ is a term dependent on $n \in \NN$.
\end{ex}

\begin{exc} If you've heard the word ``section'' before you should think about what it is being used here.
\end{exc}

\subsection*{Inference rules}

There are five types of inference rules that collectively describe the structural rules of dependent type theory. They are
\begin{enumerate}
\item Rules postulating that judgmental equality is an equivalence relation:
\[
\inferrule{\Gamma \vdash A \univ}{\Gamma \vdash A \doteq A \univ}\quad
\inferrule{\Gamma \vdash A \doteq B \univ}{\Gamma \vdash B \doteq A \univ}\quad
\inferrule{\Gamma \vdash A \doteq B \univ \\ \Gamma \vdash B \doteq C \univ}{\Gamma \vdash A \doteq C \univ}
\]
and similarly for judgmental equality between terms.
\item Variable conversion rules for judgmental equality between types:
\[
\inferrule{\Gamma \vdash A \doteq A' \univ \\ \Gamma, x : A, \Delta \vdash  \judgment}{\Gamma, x : A', \Delta \vdash \judgment}
\]

\item Substitution rules:
\[
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, \Delta \vdash \judgment}{\Gamma, \Delta[a/x] \vdash \judgment[a/x]}
\]
If $\Delta$ is the context $y_1 : B_1(x),\ldots, y_n : B_n(x,y_1,\ldots, y_{n-1})$ then $\Delta[a/x]$ is the context $y_1 : B(a), \ldots, y_n : B_n(a,y_1,\ldots, y_{n-1})$. A similar substitution is performed in the judgment $\judgment[a/x]$. Further rules indicate that substitution by judgmentally equal terms gives judgmentally equal results.
\item Weakening rules:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma,\Delta \vdash \judgment}{\Gamma, x : A, \Delta \vdash \judgment}
\]
Eg if $A$ and $B$ are types in context $\Gamma$, then $B$ is also a type in context $\Gamma, x : A$.
\item The generic term:
\[ 
\inferrule{\Gamma \vdash A \univ }{\Gamma, x : A \vdash x :A}
\]
This will be used to define the identity function of any type.
\end{enumerate}

\subsection*{Derivations}

A derivation in type theory is a finite rooted tree where each node is a valid rule of inference. The root is the conclusion.

\begin{ex} The interchange rule is derived as follows
\[
\inferrule{ 
\inferrule{ 
\inferrule{ \Gamma \vdash B \univ}{\Gamma, y :B \vdash y : B}}
{\Gamma, y : B, x : A\vdash y : B} \qquad
\inferrule{
\inferrule{}{\Gamma \vdash B \univ} \\ \inferrule{\Gamma, x : A, y : B, \Delta \vdash \judgment}{\Gamma, x : A, z : B, \Delta[z/y] \vdash \judgment[z/y]}}
{\Gamma, y : B, x : A, z : B, \Delta[z/y] \vdash \judgment[z/y]}
}{\Gamma, y : B, x : A, \Delta \vdash \judgment}
\]
\end{ex}

\section*{September 1: Dependent function types \& the natural numbers}

\subsection*{The rules for dependent function types}

Consider a section $b$ of a family $B$ over $A$ in context $\Gamma$, as encoded by a judgment:
\[ \Gamma, x : A \vdash b(x) : B(x).\]
We think of the section $b$ as a function that takes as input $x : A$ and produces a term $b(x) : B(x)$. Since the type of the output is allowed to depend on the term being input, this isn't quite an ordinary function but a \textbf{dependent function}. The type of all dependent functions is the \textbf{dependent function type}
\[ \Pi_{x : A} B(x)\]

What is a thing in mathematics? Structuralism says the ontology of a thing is determined by its behavior. In dependent type theory, we define dependent function types by stating their rules, which have the following forms:
\begin{enumerate}
\item \textbf{formation rules} tell us how a type may be formed
\item \textbf{introduction rules}  tell us how to introduce new terms of the type
\item \textbf{elimination rules}  tell us how the terms of a type may be used
\item  \textbf{computation rules} tell us how the introduction and elimination rules interact
\end{enumerate}
There are also \textbf{congruence rules} that tell us that all constructions respect judgmental equality. See \cite{Rijke} for more details.

\begin{defn}[dependent function types]
The $\Pi$-\textbf{formation rule} has the form:
\[
\inferrule{ \Gamma, x : A \vdash B(x) \univ}{\Gamma \vdash \Pi_{x :A} B(x) \univ}
\]

The $\Pi$-\textbf{introduction rule} has the form:
\[
\inferrule{ \Gamma, x : A \vdash b(x) : B(x)}{ \Gamma \vdash \lambda x. b(x) : \Pi_{x :A} B(x)}
\]
The $\lambda$-\textbf{abstraction} $\lambda x. b(x)$ can be thought of as notation for $x \mapsto b(x)$. 

The $\Pi$-\textbf{elimination rule} has the form of the evaluation function:
\[
\inferrule{ \Gamma \vdash f : \Pi_{x :A} B(x)}{\Gamma, x :A \vdash f(x) : B(x)}
\]

Finally, there are two computation rules: the $\beta$-\textbf{rule}
\[
\inferrule{\Gamma , x :A \vdash b(x) : B(x) }{ \Gamma, x : A \vdash (\lambda y. b(y))(x) \doteq b(x) : B(x)}
\]
and the $\eta$-\textbf{rule}, which says that all elements of a $\Pi$-type are dependent functions:
\[
\inferrule{ \Gamma \vdash f : \Pi_{x : A}B(x)}{\Gamma \vdash \lambda x. f(x) \doteq f : \Pi_{x :A} B(x)}\]
\end{defn}





\subsection*{Ordinary function types}

\begin{defn}[function types]
The formation rule is derived from the formation rule for $\Pi$-types together with weakening:
\[
\inferrule{
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ}{\Gamma, x : A \vdash B \univ}}
{\Gamma \vdash \Pi_{x : A}B \univ}
\]
We adopt the notation
\[ A \to B \coloneq \Pi_{x : A} B\]
for the dependent function type in the case where the type family $B$ is constant over $x : A$.

The introduction, evaluation, and computation rules are instances of term conversion: eg
\[
\inferrule{ \Gamma \vdash B \univ \\ \Gamma, x : A \vdash b(x) : B}
{ \Gamma \vdash \lambda x. b(x) : A \to B}
\qquad
\inferrule{ \Gamma \vdash f : A \to B}{\Gamma, x : A \vdash f(x) : B}
\]
plus the two computation rules:
\[
\inferrule{ \Gamma \vdash B \univ \\ \Gamma, x : A \vdash b(x) : B}{ \Gamma, x : A \vdash (\lambda y. b(y))(x) \doteq b(x) : B}
\qquad
\inferrule{ \Gamma \vdash f : A  \to B}{\Gamma \vdash \lambda x. f(x) \doteq f : A \to B}
\]
\end{defn}

\begin{defn} Identity functions are defined as follows:
\[
\inferrule{
\inferrule{ \Gamma \vdash A \univ}{\Gamma, x : A \vdash x : A}}
{ \Gamma \vdash \lambda x. x : A \to A}\]
which is traditionally denoted by $\id_A \coloneq \lambda x. x$.
\end{defn}

The idea of composition is that given a function $f \colon A \to B$ and $g \colon B \to C$ you should get a function $g \circ f \colon A \to C$. Using infix notation you might denote this function by $\_\circ\_$.

\begin{q} $\_\circ\_$ is itself a function, so it's a term of some type. What type?\footnote{Really the type should involve three universe variables but let's save this for next week.}
\end{q}

\begin{defn} Composition has the form:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash C \univ}{\Gamma \vdash \_\circ\_ : (B \to C) \to ((A \to B) \to (A \to C))}
\]
It is defined by
\[ \_\circ\_ \coloneq \lambda g. \lambda f . \lambda x. g(f(x))\]
which can be understood as the term constructed by three applications of the $\Pi$-introduction rule followed by two applications of the $\Pi$-elimination rule.
\end{defn}

Composition is associative essentially because both $(h\circ g)\circ f$ and $h \circ (g \circ f)$ are defined by $\lambda x. h(g(f(x)))$. We'll think about this more formally when we come back to identity types.

Similarly, you can compute that for all $f : A \to B$, $\id_B \circ f \doteq f : A \to B$ and $f \circ \id_A \doteq f : A \to B$.

\subsection*{The type of natural numbers}

The type $\bN$ of natural numbers is the archetypical example of an \textbf{inductive type} about more which soon. It is given by rules which say that it has a term $0_\bN : \bN$, it has a successor function $\suc : \bN \to\bN$ and it satisfies the induction principle.

The $\bN$-formation rule is
\[
\inferrule{~}{\vdash \bN \univ}
\]
In other words, $\bN$ is a type in the empty context.

There are two $\bN$-introduction rules:
\[
\inferrule{~}{\vdash 0_\bN : \bN} \qquad
\inferrule{~}{\vdash \suc : \bN \to \bN}
\]

\begin{dig}[traditional induction]
In traditional first-order logic, the principle of $\bN$-induction is stated in terms of a \textbf{predicate} $P$ over $\bN$. One way to think about $P$ is as a function $P \colon \bN \to \{ \top, \bot\}$. That is, for each $n \in \bN$, $P(n)$ is either true or false. We could also think of $P$ as an indexed family of sets $(P(n))_{n \in \bN}$ where for each $n$ either $P(n) = \emptyset$ (corresponding to $P(n)$ being false) or $P(n) = *$ (corresponding to $P(n)$ being true).

The induction principle then says \[ \forall P : \{0,1\}^\bN, (P(0) \wedge (\forall n, P(n) \to P(n+1)) \to \forall n, P(n)).\]
\end{dig}

In dependent type theory it is most natural to let $P$ be an arbitrary type family over $\bN$. This is a stronger assumption, as we'll see. 

\begin{q}
What then corresponds to a proof that $\forall n, P(n)$?
\end{q}

The induction principle is encoded by the following rule:
\[
\inferrule{ \Gamma, n : \bN \vdash P(n) \univ \\ \Gamma \vdash p_0 : P(0_\bN) \\ \Gamma \vdash p_S : \Pi_{n : \bN} (P(n) \to P(\suc(n))) }
{ \Gamma \vdash \term{ind}_\bN(p_0,p_S) : \Pi_{n : \bN} P(n)}
\]

\begin{rmk} There are other forms this rule might take that are interderivable with this one.
\end{rmk}

The computation rules say that the function $\term{ind}_\bN(p_0,p_S) : \Pi_{n : \bN} P(n)$ behaves like it should on $0_\bN$ and successors:
\[
\inferrule{ \Gamma , n : \bN \vdash P(n) \univ \\ \Gamma \vdash p_0 : P(0_\bN) \\ \Gamma \vdash p_S : \Pi_{n : \bN} (P(n) \to P(\suc(n))) }
{ \Gamma \vdash \term{ind}_\bN(p_0,p_S)(0_\bN) \doteq p_0 : P(0_\bN)}
\]
and under the same premises
\[ \Gamma, n : \bN \vdash \texttt{ind}_\bN(p_0, p_S)(\suc(n)) \doteq p_S(n, \texttt{ind}_\bN(p_0,p_S,n)) : P(\suc(n)).\]
These computation rules don't matter so much if the type family $n : \bN \vdash P(n)$ is really a predicate --- $P(n)$ is either true or false and that's the end of the story --- but they do matter if $P(n)$ is more like an indexed family of sets. In the latter case, $\term{ind}_\bN(p_0,p_S)$ is the recursive function defined from $p_0$ and $p_S$ and these are the computation rules for that recursion.

\begin{rmk} Recall Peano's axioms for the natural numbers: 
\begin{enumerate}
\item $0_\bN \in \bN$
\item $\suc : \bN \to \bN$
\item $\forall n, \suc(n) \neq 0_\bN$
\item $\forall n,m, \suc(n)= \suc(m) \to n=m$
\item induction
\end{enumerate}
We'll be able to \emph{prove} the missing two axioms from the induction principle we've assumed once we have identity types and universes. We'll come back to this in a few weeks.
\end{rmk}

\subsection*{Addition on the natural numbers}

\begin{rmk} When addition is defined by recursion on the second variable, from the computation rules associated to function types and the natural numbers type you can derive judgmental equalities \[
m + 0 \doteq m \quad \text{and} \quad m + \suc(n) \doteq \suc(m + n).
\]
But you can't derive the symmetric judgmental equalities.
\end{rmk}

We \emph{will} be able to prove such equalities using the identity types, to be introduced shortly.

\subsection*{Pattern matching}

To define a dependent function $f : \Pi_{n : \bN} P(n)$ by induction on $n$ it suffices, by the elimination rule for the natural numbers type, to provide two terms:
\[ p_0 : P(0_\bN) \qquad p_S : \Pi_{n : \bN} P(n) \to P(\suc(n)).\]
Thus the definition of $f$ may be presented by writing
\[ f(0_\bN) \coloneq p_0 \qquad f(\suc(n)) \coloneq p_S(n,f(n)).\]
This defines the function $f$ by \textbf{pattern matching} on the variable $n$. When a function is defined in this form, the judgmental equalities accompanying the definition are immediately displayed.

\section*{September 8: The formal proof assistant \texttt{agda}}

See \url{https://github.com/emilyriehl/721/blob/master/introduction.agda}


\section*{September 13: Inductive types}

The rules for the natural numbers type $\bN$ tell us:
\begin{enumerate}
\item how to form terms in $\bN$, and
\item how to define dependent functions in $\Pi_{n : \bN} P(n)$ for any type family $n : \bN \vdash P(n) \univ$,
\end{enumerate}
while providing two computation rules for those dependent functions.

Many types can be specified by stating how to form their terms and how to define dependent functions out of them. Such types are called \textbf{inductive types}. 

\subsection*{The idea of inductive types}

Recall a type is specified by its formation rules, its introduction rules, its elimination rules, and its computation rules. For inductive types, the introduction rules specify the \textbf{constructors} of the inductive type, while the elimination rule provides the \textbf{induction principle}. The computation rules provide definitional equalities for the induction principle.

In more detail:
\begin{enumerate}
\item The constructors tell us what structure the identity type is given with.
\item The induction principle defines sections of any type family over the inductive type by specifying the behavior at the constructors.
\item The computation rules assert that the inductively defined section agrees on the constructors with the data used to define it. So there is one computation rule for each constructor.
\end{enumerate}

\subsection*{The unit type}

The formal definition of the \textbf{unit type} is as follows:
\[
\inferrule{}{\vdash \1 \univ} \qquad
\inferrule{}{\vdash \star : \1} \qquad 
\inferrule{ x : \1 \vdash P(x) \univ \\ p : P(\star)}{ x: \1 \vdash \ind_\1(p,x) : P(x)} \qquad
\inferrule{ x : \1 \vdash P(x) \univ \\ p : P(\star)}{x : \1 \vdash \ind_1(p,\star) \doteq p : P(\star)}
\]
As an inductive type, the definition is packaged as follows:

\begin{defn} The \textbf{unit type} is a type $\1$ equipped with a term $\star : \1$ satisfying the inductive principle that for any family $x : \1 \vdash P(x)$ there is a function
\[ \ind_\1 : P(\star) \to \Pi_{x : 1} P(x)\]
with the computation rule $\ind_1(p,\star) \doteq p$.
\end{defn}

In agda, this definition has the form:
\begin{quote}
\texttt{data unit : UU lzero where\\ \indent
  star : unit}
  \end{quote}


\begin{q} What does the induction rule look like for a constant type family $A$ that does not depend on $\1$?
\end{q}

\subsection*{The empty type}

\begin{defn}
The empty type is a type $\emptyset$ satisfying the induction principle that for any family of types $x : \emptyset \vdash P(x)$ there is a term
\[ \ind_\emptyset : \Pi_{x : \emptyset} P(x).\]
\end{defn}

That is the empty type is the inductive type with no constructors. Thus there are no computation rules. In agda, this definition has the form:
\begin{quote}
\texttt{data empty : UU lzero where}
  \end{quote}

\begin{rmk} As a special case of the elimination rule for the empty type we have
\[
\inferrule{ \vdash A \univ}{ \term{ex-falso} \coloneq \ind_\emptyset : \emptyset \to A}
\]
By the elimination rule for function types it follows that if we had a term $x : \emptyset$ then we could get a term in any type. The name comes from latin \emph{ex falso quodlibet}: ``from falsehood, anything.''
\end{rmk}

We've already seen a few glimpses of logic in type theory, something we'll discuss more formally soon. The basic idea is that we can interpret the formation of a type as akin to the process of formulating a mathematical statement that could be a sentence (if its a type in the empty context) or a predicate (if it's a dependent type). The act of constructing a term in that type is then analogous to proving the proposition so-encoded. 
These ideas motivate the logically-inflected terms in what follows. 

For instance, we can use the empty type to define a negation operation on types:
\begin{defn}
For any type $A$, we define its \textbf{negation} by $\neg A \coloneq A \to \emptyset$ and say the type $A$ \textbf{is empty} if there is a term in this type.
\end{defn}

\begin{rmk} To construct a term of type $\neg A$, use the introduction rule for function types and assume given a term $a : A$. The task then is to derive a term of $\emptyset$. In other words, we prove $\neg A$ by assuming $A$ and deriving a contradiction. This proof technique is called \textbf{proof of negation}.

This should be contrasted with \textbf{proof by contradiction}, which aims to prove a proposition $P$ by assuming $\neg P$ and deriving a contradiction. This uses the logical step ``$\neg \neg P$ implies $P$.'' In type theory, however, $\neg \neg A$ is the type of functions
\[ \neg \neg A \coloneq (A \to \emptyset) \to \emptyset)\] and it is not possible in general to use a term in this type to construct a term of type $A$. 
\end{rmk}

The law of contraposition does work, at least in one direction.

\begin{prop} For any types $P$ and $Q$ there is a function
\[ (P \to Q) \to (\neg Q \to \neg P).\]
\end{prop}
\begin{proof}
By $\lambda$-abstraction assume given $f : P \to Q$ and $\tilde{q} : Q \to \emptyset$. We seek a term in $P \to \emptyset$, which we obtain simply by composing: $\tilde{q} \circ f : P \to \emptyset$. Thus
\[\lambda f. \lambda \tilde{q} .\lambda p. \tilde{q}(f(p)) : (P \to Q) \to (\neg Q \to \neg P). \qedhere\]
\end{proof}

\subsection*{Coproducts}

Inductive types can be defined outside the empty context. For instance, the formation and introduction rules for the coproduct type have the form:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ}{\Gamma \vdash A + B \univ} 
\]
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash a : A}{ \Gamma \vdash \inl a : A + B} \qquad
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash b : B}{ \Gamma \vdash \inr b : A + B}
\]

\begin{defn} Given types $A$ and $B$ the \textbf{coproduct type} is the type equipped with 
\[ \inl : A \to A + B \qquad \inr : B \to A+ B\]
satisfying the induction principle that says that for any family of types $x : A + B \vdash P(x) \univ$ there is a term
\[ \ind_+ : \left( \Pi_{x : A} P(\inl (x))\right) \to \left( \Pi_{y : B} P(\inr(y))\right) \to \Pi_{z : A + B}P(z)\]
satisfying the computation rules
\[ \ind_+(f,g, \inl(x)) \doteq f(x) \qquad \ind_+(f,g, \inr(y)) \doteq g(y).\]
\end{defn}
Not as a special case we have
\[ \ind_+ : (A \to X) \to (B \to X) \to (A + B \to X)\]
which is similar to the elimination rule for disjunction in first order logic: if you've proven that $A$ implies $X$ and that $B$ implies $X$ then you can conclude that $A$ or $B$ implies $X$.

\subsection*{The type of integers}

There are many ways to define the integers in Martin-L\"{o}f type theory, one of which is as follows:

\begin{defn} Define the \textbf{integers} to be the type $\bZ \coloneq \bN + (\1 + \bN)$ which comes equipped with inclusions:
\[ \term{in-pos} \coloneq \inr\circ\inr : \bN \to \bZ \qquad \term{in-neg} \coloneq \inl : \bN \to \bZ\]
and constants
\[ -1_\bZ \coloneq \term{in-neg}(0_\bN) \qquad 0_\bZ \coloneq \inr (\inl(\star)) \qquad 1_{\bZ} \coloneq \term{in-pos}(0_\bN).\]
\end{defn}

Since $\bZ$ is built from inductive types it is then an inductive type given with its own induction principle.

\subsection*{Dependent pair types}

Of all the inductive types we've introduced, the final one is perhaps the most important.

Recall a \textbf{dependent function} $\lambda x . f(x) : \Pi_{x : A}B(x)$ is like an ordinary function except the output type is allowed to vary with the input term. Similarly, a \textbf{dependent pair} $(a,b) : \Sigma_{x : A}B(x)$ is like an ordinary (ordered) pair except the type of the second term $b : B(a)$ is allowed to vary with the first term $a :A$.

\begin{defn} Consider a type family $x : A \vdash B(x) \univ$. The \textbf{dependent pair type} or $\Sigma$-\textbf{type} $\Sigma_{x :A}B(x)$ is the inductive type equipped with the function
\[ \pair : \Pi_{x : A} \left(B(x) \to \Sigma_{y : A}B(y)\right).\]
The induction principle asserts that for any family of types $p : \Sigma_{x :A} B(x) \vdash P(p) \univ$ there is a function
\[ \ind_\Sigma : \left( \Pi_{x :A} \Pi_{y : B}P(\pair(x,y) \right) \to \left( \Pi_{z : \Sigma_{x :A}B(x)}P(z) \right)
\]
satisfying the computation rule $\ind_\Sigma (g,\pair(x,y)) \doteq g(x,y)$.
\end{defn}

It is common to write ``$(x,y)$'' as shorthand for ``$\pair(x,y)$.''

\begin{defn} Given a type family $x :A \vdash B(x) \univ$ by the induction principle for $\Sigma$-types, we have a function
\[ \pr_1 : \Sigma_{x :A}B(x) \to A\]
defined by $\pr_1(x,y) \coloneq x$ and a dependent function
\[ \pr_2 :  \Pi_{ p : \Sigma_{x :A}B(x)} B(\pr_1(p))\]
defined by $\pr_2(x,y) \coloneq y$.
\end{defn}

When $B$ is a constant type family over $A$, the type $\Sigma_{x : A}B$ is the type of ordinary pairs $(x,y)$ where $x :A $ and $y: B$. Thus \textbf{product types} arise as special cases of $\Sigma$-types.

\begin{defn} Given types $A$ and $B$ their product type is the type $A \times B \coloneq \Sigma_{x :A}B$. It comes with a pairing function
\[ (-,-) : A \to B \to A \times B\] and satisfies an induction principle:
\[ \ind_\times : \Pi_{x : A} \Pi_{y : B} P(x,y) \to \Pi_{z : A \times B} P(z)\]
satisfying the computation rule $\ind_\times (g,(x,y)) \doteq g(x,y)$.
\end{defn}

As a special case, we have
\[ \ind_\times : (A \to B \to C) \to ((A \times B) \to C).\]
This is the inverse of the \textbf{currying function}. Thus $\ind_\times$ and $\ind_\Sigma$ sometimes go by the name \textbf{uncurrying}.

\section*{September 15: Identity types}

We have started to develop an analogy in which types play the role of mathematical propositions and terms in a type play the role of proofs of that proposition. More exactly, we might think of a type as a ``proof-relevant'' proposition, the distinction being that the individual proofs of a given proposition---the terms of the type---are first class mathematical objects, which may be used as ingredients in future proofs, rather than mere witnesses to the truth of the particular proposition.

The various constructions on types that we have discussed are analogous to the logical operations ``and,'' ``or,'' ``implies,'' ``not,'' ``there exists,'' and ``for all.'' We also have the unit type $\1$ to represent the proposition $\top$ and the empty type $\emptyset$ to represent the proposition $\bot$. There is one further ingredient from first-order logic that is missing a counterpart in dependent type theory: the logical operation ``$=$.''

Given a type $A$ and two terms $x,y : A$ it is sensible to ask whether $x = y$. From the point of view of types as proof-relevant propositions, ``$x=y$'' should be the name of a type, in fact a dependent type. The formation rule for \textbf{identity types} says
\[
\inferrule{ \Gamma \vdash A \univ}
{\Gamma, x : A, y : A \vdash x =_A y \univ}
\]
where ``$x=y$'' is commonly used as an abbreviation for ``$x=_Ay$'' when the type of $x$ and $y$ is clear from context. A term $p : x = y$ of an identity type is called an \textbf{identification} of $x$ and $y$ or a \textbf{path} from $x$ to $y$ (more about this second term later). Identifications have a rich structure that follows from a very simple characterization of the identity type due to Per Martin-L\"{o}f: it is the inductive type family freely generated by the reflexivity terms.

\subsection*{The inductive definition of identity types}

We can define identity types as inductive types in either a one-sided or two-sided fashion. The induction rule may be easier to understand from the one-sided point of view, so we present it first.

\begin{defn}[one-sided identity types] Given a type $A$ and a term $a : A$, the \textbf{identity type} of $A$ at $a$ is the inductive family of types $x : A \vdash a =_A x \univ$ with a single constructor $\refl_a : a =_A a$. The induction principle is postulates that for any type family $x : A, p : a =_A x \vdash P(x,p) \univ$ there is a function
\[ \pathind_a : P(a, \refl_a) \to \Pi_{x : A} \Pi_{p : a =_A x} P(x,p)\]
satisfying $\pathind_a(q,a, \refl_a) \doteq q$.
\end{defn}

This is a very strong induction principle: it says that to prove a predicate $P(x,p)$ depending on any term $x :A$ and any identification $p : a =_A x$ it suffices to assume $x$ is $a$ and $p$ is $\refl_a$ and prove $P(a,\refl_a)$. 

More formally, identity types are defined by the following rules:
\[
\inferrule{ \Gamma \vdash a : A}{ \Gamma, x : A \vdash a =_A x \univ} \qquad
\inferrule{\Gamma \vdash a : A}{\Gamma \vdash \refl_a : a =_A a}\]
\[ 
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, p: a=_A x \vdash P(x,p) \univ}{\Gamma \vdash \pathind_a : P(a,\refl_a) \to \Pi_{x :A}\Pi_{p : a =_A x}P(x,p)} \qquad
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, p: a=_A x \vdash P(x,p) \univ}{\Gamma \vdash \pathind_a(q,a, \refl_a) \doteq q : P(a,\refl_a)}
\]

Equally, the identity type can be considered in a two-sided fashion:
\begin{defn}[two-sided identity types]
 Given a type $A$, the \textbf{identity type} of $A$ is the inductive family of types $x : A, y :A \vdash x =_A y \univ$ with a single constructor $x : A \vdash \refl_x : x =_A x$. The induction principle is postulates that for any type family $x : A, y : A, p : x =_A y \vdash P(x,y,p) \univ$ there is a function
\[ \pathind : \Pi_{a : A} P(a, a, \refl_a) \to \Pi_{x : A} \Pi_{y: A}\Pi_{p : x =_A y} P(x,y,p)\]
satisfying $\pathind(q,a, a,\refl_a) \doteq q$.
\end{defn}

In this form, the identity types are defined by the following rules:
\[
\inferrule{ \Gamma \vdash A}{ \Gamma, x : A, y : A \vdash x =_A y \univ} \qquad
\inferrule{\Gamma \vdash A}{\Gamma, x :A \vdash \refl_x : x =_A x}\]
\[ 
\inferrule{\Gamma \vdash A \\ \Gamma, x : A, y : A, p: x=_A y \vdash P(x,y,p) \univ}{\Gamma \vdash \pathind : \Pi_{a : A} P(a,a,\refl_a) \to \Pi_{x :A}\Pi_{y :A}\Pi_{p : x =_A y}P(x,y, p)} \quad
\inferrule{\Gamma \vdash  A \\ \Gamma, x : A, y : A, p: x=_A y \vdash P(x,y,p) \univ}{\Gamma, a : A \vdash  \pathind (q,a,a,\refl_a) \doteq q : P(a,a,\refl_a)}
\]

These presentations are interderivable.

\subsection*{The groupoid structure on types}

Mathematical equality, as traditionally understood, is an equivalence relation: it's reflexive, symmetric, and transitive. But all we've asserted about identity types is that they are inductively generated by the reflexivity terms! As we'll now start to discover, considerable additional structure follows.

\begin{prop}[symmetry] For any type $A$, three is an  \textbf{inverse operation}
\[ \inv : \Pi_{x, y :A} x = y \to y =x.\]
\end{prop}
\begin{proof}
We define $\inv$ by path induction. By the introduction rule for function types it suffices to define $\inv p : y =x$ for $p : x = y$. Consider the type family $x : A, y : A, p: x = y \vdash P(x,y,p) \coloneq y = x$. By path induction to inhabit $y=x$ it suffices to assume $x = y$ and $p$ is $\refl_x$ in which case we may define $\inv \refl_x \coloneq \refl_x : x=x$. Thus $\inv$ is
\[ \pathind (\lambda x, \refl x) : \Pi_{x :A} \Pi_{y :A} \Pi_{x = y} y =x.\]
\end{proof}

\begin{ntn} Write $p^{-1}$ for $\inv(p)$.
\end{ntn}

\begin{prop}[transitivity]
For any type $A$, there is a  \textbf{concatenation} operation
\[ \concat : \Pi_{x,y,z :A} x= y \to y=z \to x = z.\] 
\end{prop}
\begin{proof}
We define $\concat$ by appealing to the path induction principle for identity types. 
By the introduction rule for dependent function types, to define $\concat$ you may assume given $p : x=y$. The task is then to define $\concat (p) : \Pi_{z} y =z \to x = z$.  For this, consider the type family $x :A , y : A, p : x = y \vdash P(x,y,p)$ where $P(x,y,p) \coloneq \Pi_{z : A} (y = z) \to (x = z)$. By applying the function $\pathind$ to get a term of this type it suffices to assume $y$ is $x$ and $p$ is $\refl_x$. So we need only define $\concat( \refl_x) : \Pi_{z :A} x =z \to x = z$ and we define this to be the identity function $\id_{x = z}$. Thus the function $\concat$ is 
\[  \pathind( \lambda x, \lambda z, \id_{x=z}) : \Pi_{x :A}\Pi_{y :A} \Pi_{p : x = y} \Pi_{z :A} y=z \to x =z,\]
which can be regarded as a function in the type $\Pi_{x,y,z :A} x= y \to y=z \to x = z$ by swapping the order of the arguments $p$ and $z$.
\end{proof}

\begin{ntn} Write $p \cdot q$ for $\concat(p,q)$.
\end{ntn}

While the elimination rule for identity types is quite strong the corresponding computation rule is relatively weak. It's not strong enough to show that $(p \cdot q) \cdot r$ and $p \cdot (q \cdot r)$ are judgmentally equal for any $p : x = y$, $q : y = z$, and $r : z = q$. In fact there are countermodels that show that this is false in general. However, since both $(p \cdot q) \cdot r$ and $p \cdot (q \cdot r)$ are terms of type $x = w$ we can ask whether there is an identification between them and it turns out this is always true.

\begin{prop}[associativity] Given $x,y,z,w  :A$ and identifications $p : x = y$, $q : y =z$, and $r : z = w$, there is an associator
\[ \assoc (p,q,r) : (p \cdot q) \cdot r = p \cdot (q \cdot r)\]
\end{prop}
\begin{proof} We define $\assoc(p,q,r)$ by path induction. 

Consider the type family $x :A, y : A, p: x = y \vdash \Pi_{z :A} \Pi_{q : y =z} \Pi_{w:A} \Pi_{r : z =w} (p \cdot q) \cdot r = p \cdot (q \cdot r)$. To define a term $\assoc (p,q,r)$ in here it suffices to assume $y$ is $x$ and $p$ is $\refl_x$ and define
\[ \lambda z. \lambda q. \lambda w. \lambda r. \assoc(\refl_x, q,r) : \Pi_{z :A} \Pi_{q : x =z} \Pi_{w:A} \Pi_{r : z =w} (\refl_x \cdot q) \cdot r = \refl_x \cdot (q \cdot r).\]
By the definition of concatenation, $\refl_x \cdot q \doteq q$ and $\refl_x \cdot (q \cdot r) \doteq q \cdot r$. So we must define 
\[ \assoc(\refl_x, q,r) :  q \cdot r = q \cdot r\]
and we can take this term to be $\refl_{q \cdot r}$.
\end{proof}

\begin{prop}[units] For any type $A$, there are left and right \textbf{unit laws}
\[ \lambda x. \lambda y. \lambda p. \term{left-unit}(p) :  \lambda {x,y : A} \Pi_{p: x = y} \refl_x \cdot p = p \qquad  \lambda x. \lambda y. \lambda p.\term{right-unit}(p) : \Pi_{x , y : A} \Pi_{p : x = y} p \cdot \refl_y = p.\]
\end{prop}
\begin{proof}
We are asked to define  dependent functions that takes $x, y :A$ and $p : x = y$ and produce terms
\[ \term{left-unit}(p) : \refl_x \cdot p = p \qquad \term{right-unit}(p) : p \cdot \refl_y = p.\]
By path induction, it suffices to assume $y$ is $x$ and $p$ is $\refl_x$, in which case we require terms
\[ \term{left-unit}(\refl_x) : \refl_x \cdot \refl_x = \refl_x \qquad \term{right-unit}(\refl_x) : \refl_x \cdot \refl_x = \refl_x.\] By the definition of concatenation $\refl_x \cdot \refl_x \doteq \refl_x$ so we can take $\refl_{\refl_x}$ as both $\term{left-unit}(\refl_x)$ and $\term{right-unit}(\refl_x)$.
\end{proof}

\begin{prop}[inverses] For any type $A$, there are left and right \textbf{inverse laws}
\[  \lambda x. \lambda y. \lambda p.\term{left-inv}(p) : \Pi_{x,y :A} \Pi_{p : x= y} p^{-1} \cdot p = \refl_y \qquad  \lambda x. \lambda y. \lambda p.\term{right-inv}(p) : \Pi_{x,y :A} \Pi_{p : x=y} p \cdot p^{-1} = \refl_x.\]
\end{prop}
\begin{proof}
We are asked to define  dependent functions that takes $x, y :A$ and $p : x = y$ and produce terms
\[ \term{left-inv}(p) : p^{-1} \cdot p = \refl_y \qquad \term{right-inv}(p) : p \cdot p^{-1} = \refl_x.\]
By path induction, it suffices to assume $y$ is $x$ and $p$ is $\refl_x$, in which case we require terms
\[ \term{left-inv}(\refl_x) : \refl_x^{-1} \cdot \refl_x = \refl_x \qquad \term{right-inv}(\refl_x) : \refl_x \cdot \refl_x^{-1} = \refl_x.\]
By the definitions of concatenation and inverses, again both left-hand and right-hand sides are judgementally equal so we take $\term{left-inv}(\refl_x)$ and $\term{right-inv}(\refl_x)$ to be $\refl_{\refl_x}$.
\end{proof}

\section*{September 20: More identity types}

\subsection*{Types as \texorpdfstring{$\infty$}{infinity}-groupoids}

Martin-L\"{o}f's rules for the identity types date from a 1975 paper ``An Intuitionistic Theory of Types.'' In the following two decades, there was a conjecture that went by the name ``uniqueness of identity proofs" that for any $x,y : A$, $p, q  : x =_A y$, the type $p =_{x=_A y} q$ is inhabited, meaning that it's possible to construct an identification between $p$ and $q$. In 1994, Martin Hofmann and Thomas Streicher constructed a model of Martin-L\"{o}f's dependent type theory in the category of groupoids that refutes uniqueness of identity proofs.\footnote{The technical details of what exactly it means to ``construct a model of type theory'' are quite elaborate and would be interesting to explore as a final project.}

In the Hofmann-Streicher model, types $A$ correspond to \emph{groupoids} and terms $x, y : A$ correspond to \emph{objects} in the groupoid. An identification $p : x = y$ corresponds to a(n iso)morphism $p : x \to y$ in the groupoid, while an identification between identifications exists if and only if $p$ and $q$ define the same morphism. Since there are groupoids with multiple distinct morphisms between a fixed pair of objects, we see that it is not always the case that $p=_{x=_A y} q$. Following Hofmann-Streicher, it made sense to start viewing types as more akin to groupoids than to sets. The proofs of symmetry and transitivity for identity types are more accurately described as inverses and concatenation operations in a groupoid. As we've seen, these satisfy various associativity, unit, and inverse laws---up to identification at least---as required by a groupoid.

But that last caveat is important. We've shown that for any type $A$, its identity types $x,y :A \vdash x=_A y \univ$ give it something like the structure of a groupoid. But for each $x,y :A$, $x=_A y$ is also a type, so \emph{its} identity types $p,q: x=_A y \vdash p =_{x=_A y}q \univ$ give $x=_A y$ its own groupoid structure. And the higher identity types, $\alpha, \beta : p =_{x=_A y}q  \vdash \alpha= \beta \univ$ give $p =_{x=_A y}q$ its own groupoid structure and so on. So a modern point of view is that the types in Martin-L\"{o}f's dependent type theory should be thought of as $\infty$-\emph{groupoids}.

If $A$ is an $\infty$-groupoid, its terms $x :A$ might be called \textbf{points} and its identifications $p : x =_A y$ might be called \textbf{paths}. This explains the modern name ``path induction'' for the induction principle for identity types. These ideas are at the heart of the homotopical interpretation of type theory, about more which later.

\subsection*{The uniqueness of \texorpdfstring{$\refl$}{refl}}

The definition of the identity types says that the family of types $a =x$ indexed by $x : A$ is inductively generated by the term $\refl_a : a = a$. It does \emph{not} say that the type $a =a$ is inductively generated by $a : A$. In particular, we cannot apply path induction to prove that $p = \refl_a$ for any $p : a = a$ because in this case neither endpoint of the identity type is free. 

There is a sense however in which the reflexivity term is unique:

\begin{prop} For any type $A$ and $a : A$, $(a, \refl_a)$ is the unique term of the type $\Sigma_{x : A} a =x$. That is, for any $z : \Sigma_{x :A} a =x$, there is an identification $(a, \refl_a) = z$.
\end{prop}
\begin{proof} We're trying to define a dependent function that takes $z : \Sigma_{x :A} a =x$ and gives a term in the identity type $(a,\refl_a) =_{\Sigma_{x :A} a =x} z$. By $\Sigma$-induction it suffices to assume $z$ is a pair $(x,p)$ where $x :A$ and $p : a =x$ and construct an identification $(a, \refl_a) =_{\Sigma_{x :A} a =x} (x,p)$. So now we're trying to define a dependent function that takes $x :A$ and $p : a =x$ and constructs an identification $(a, \refl_a) =_{\Sigma_{x :A} a =x} (x,p)$. By path induction, it suffices to assume $x$ is $a$ and $p$ is $\refl_a$. But now we can use reflexivity to show that $(a, \refl_a) = (a, \refl_a)$.
\end{proof}

In terminology to be introduced later, this result says that the type $\Sigma_{x : A} a =x$ is \textbf{contractible} with the term  $(a, \refl_a)$ serving as its \textbf{center of contraction}.


\subsection*{The action of paths on functions}

The structural rules of type theory guarantee that any function (and indeed any construction in type theory) preserve definitional equality. We now show that in addition every function preserves identifications.

\begin{prop} Let $f \colon A \to B$. There is an operation that defines the \textbf{action on paths} of $f$
\[ \ap_f : \Pi_{x,y :A} (x=y) \to (f(x) = f(y))\]
that satisfies the coherence conditions
\[ \apcoh{id}_A : \Pi_{x,y:A} \Pi_{p : x= y} p = \ap_{\id_A}(p)\]
\[ \apcoh{comp}(f,g) : \Pi_{x,y:A} \Pi_{p: x=y} \ap_g(\ap_f(p))= \ap_{g\circ f}(p).\]
\end{prop}
\begin{proof}
By path induction to define $\ap_f(p) : f(x)=f(y)$ it suffices to assume $y$ is $x$ and $p$ is $\refl_x$. We may then define $\ap_f(\refl_x) \coloneq \refl_{f(x)} : f(x) = f(x)$.

Next to define $\apcoh{id}_A$ it similarly suffices to suppose $y$ is $x$ and $p$ is $\refl_x$. Since $\ap_{\id_A}(\refl_x)\doteq \refl_x$, we may define $\apcoh{id}_A(\refl_x) \coloneq \refl_{\refl_x} : \refl_x = \refl_x$.

Finally, to define $\apcoh{comp}(f,g)$, by path induction we may again assume $y$ is $x$ and $p$ is $\refl_x$. Since both $\ap_g(\ap_f(\refl_x))$ and $\ap_{g \circ f}(\refl_x)$ are defined to be $\refl_{g(f(x))}$ we may define $\apcoh{comp}(f,g)(\refl_x)$ to be $\refl_{\refl_{g(f(x))}}$.
\end{proof}

If the types $A$ and $B$ are thought of as $\infty$-groupoids, then $f \colon A \to B$ can be thought of as a functor of $\infty$-groupoids in a sense hinted at by the following lemma.

\begin{lem} For $f \colon A \to B$ there are identifications
\begin{align*}
\apcoh{refl}(f,x) &: \ap_f (\refl_x) = \refl_{f(x)} \\
\apcoh{inv}(f,p) &: \ap_f (p^{-1}) = \ap_f(p)^{-1} \\
\apcoh{concat}(f,p,q) &: \ap_f(p \cdot q) = \ap_f(p) \cdot \ap_f(q)
\end{align*}
for every $p : x= y$ and $q: y = z$.
\end{lem}
\begin{proof}
For the first coherence, there is a definitional equality $\ap_f (\refl_x) \doteq \refl_{f(x)}$ so we take $\apcoh{refl}(f,x) \coloneq \refl_{\refl_{f(x)}}$. 

We define $\apcoh{inv}(f,p)$ by path induction on $p$ by defining $\apcoh{inv}(f,\refl_x) \coloneq \refl_{\refl_{f(x)}}$.

Similarly, we define $\apcoh{concat}(f,p,q)$ by path induction on $p$ (since concat was defined by path induction on $p$) by defining $\apcoh{concat}(f,\refl_x,q)$ to be $\refl_{\ap_f(q)}$.
\end{proof}

\subsection*{Transport}

The term $\ap_f$ defines the action of a non-dependent function $f \colon A \to B$ on paths in $A$. It's natural to ask whether a dependent function $f : \Pi_{z:A}B(z)$ also induces an action on paths. There's a challenge here, though. If $x,y : A$ are terms belonging to the base type, then we can form the type $x=_A y$ to ask whether they are identifiable. But the terms $f(x) : B(x)$ and $f(y) : B(y)$  belong to different types and are not identifiable. But nevertheless if there is  path $p : x = y$ identifying $y$ with $x$ intuition suggests there should be some way to compare $f(y)$ to $f(x)$.

To achieve this, we must construct a different sort of action of paths function first. This is called the  \textbf{transport} function for dependent types $x :A \vdash B(x) \univ$ that, given an identification $p : x = y$ in the base type, can be used to transport any term in $B(x)$ to a term in $B(y)$.

\begin{prop} For any type family $x : A \vdash B(x) \univ$, there is a \textbf{transport operation}
\[ \tr_B : \Pi_{x,y :A} (x=y) \to (B(x) \to B(y)).\]
\end{prop}
\begin{proof}
By path induction it suffices to define $\tr_B(\refl_x)) \coloneq \id_{B(x)}$.
\end{proof}

As an application of transport we can now defined the action on paths of a dependent function.

\begin{prop} For any dependent function $f : \Pi_{z: A} B(z)$ and identification $p : x=_A y$ there is a path
\[ \apd_f(p) : \tr_B(p, f(x)) =_{B(y)} f(y).\]
\end{prop}
\begin{proof}
The function
 \[ \lambda x . \lambda y. \lambda p. \apd_f(p) : \Pi_{x,y:A} \Pi_{p : x=y} \tr_B(p, f(x)) =_{B(y)} f(y)\]
 may be defined by path induction on $p$. It suffices to construct a path
 \[ \lambda x . \apd_f(\refl_x) : \Pi_{x :A} \tr_B(\refl_x,f(x)) =_{B(x)} f(x).\]
Since  $\tr_B(\refl_x),f(x)) \doteq f(x)$ we may defined $\apd_f(\refl_x) \coloneq \refl_{f(x)}$.
\end{proof}

\subsection*{The laws of addition on $\bN$}

Recall that we defined the addition of natural numbers in such a way that
\[ m+ 0 \doteq m \qquad m+ \suc(n) \doteq \suc(m+n)\]
by induction on the second variable. With this definition, these are the only definitional equalities. However, it is possible to produce identifications proving the other commutative monoid axioms.

\begin{lem} For any $n : \bN$ there are identifications
\[\term{left-unit-law-add}_\bN(n) : 0 + n = n \qquad \term{right-unit-law-add}_\bN(n) : n+0=n.\]
\end{lem}
\begin{proof}
The second of these can be taken to be $\refl_n$ but the first is more complicated. We define $\term{left-unit-law-add}_\bN(n)$ by induction on $n : \bN$. When $n = 0$, $0+0=0$ holds by reflexivity. 

Our final goal is to show $0+\suc(n) = \suc(n)$, for which it suffices to construct an identification \[ \suc(0+n) = \suc(n)\]
by the definition of addition. We may assume we have an identification $p : 0 + n = n$. Thus, we can use the action on paths of $\suc : \bN \to \bN$  to obtain a term $\ap_{\suc}(p) : \suc(0+n) = \suc(n)$.
\end{proof}

\begin{prop} For any $m,n : \bN$ there are identifications
\begin{align*} \term{left-successor-law-add}_\bN(m,n) &: \suc(m) + n = \suc(m+n) \\
\term{right-sucessor-law-add}_\bN(m,n) &= m+ \suc(n) = \suc(m+n)
\end{align*}
\end{prop}
\begin{proof}
Again the second identification holds judgmentally so we  define \[\term{right-sucessor-law-add}_\bN(m,n) \coloneq \refl_{\suc(m+n)}.\] We construct the former using induction on $n \in \bN$. The base case $\suc(m)+0 = \suc(m+0)$ holds by $\refl_{\suc(m)}$. For the inductive step we assume we have an identification $p : \suc(m)+n = \suc(m+n)$. Our goal is to show that $\suc(m)+\suc(n) = \suc(m+\suc(n))$. By action of paths of $\suc : \bN \to \bN$ we obtain a term
\[ \ap_{\suc}(p) : \suc(\suc(m)+n) = \suc(\suc(m+n))\] but here the left hand side is judgmentally equal to $\suc(m)+\suc(n)$ while the right hand side is judgmentally equal to $\suc(m+ \suc(n))$.
\end{proof}  

\begin{prop}[associativity] For all $k,m,n : \bN$,
\[ \term{associative-add}_\bN(k,m,n) : (m+n)+k = m+(n+k).\]
\end{prop}
\begin{proof}
We construct $\term{associative-add}_\bN(k,m,n)$ by induction on $n$. In the base case we have
\[ (k+m)+0 \doteq k+m \doteq k + (m+0),\] so we define $\term{associative-add}_\bN(k,m,0) \coloneq \refl_{m+n}$.

For the inductive step let $p : (k+m)+n = k+(m+n)$. We then have
\[ \ap_{\suc}(p) : \suc((k+m)+n) = \suc(k+(m+n)).\]
We have $\suc((k+m)+n) \doteq (k+m)+\suc(n)$ and $\suc(k+(m+n)) \doteq k + \suc(m+n) \doteq k + (m + \suc(n))$ so this term is the term we wanted.
\end{proof}


\begin{prop}[commutativity] For all $m,n : \bN$,
\[ \term{commutative-add}_\bN(m,n) : m+n = n+m.\]
\end{prop}
\begin{proof} By induction on $m$ we have to show $0+n = n+0$, which holds by the unit laws for $n$. Then we may assume $p : m+n = n+m$ and must show $\suc(m)+n= n + \suc(m)$. We have
\[ \ap_{\suc}(p) : \suc(m+n) = \suc(n+m).\] We then concatenate this path with the paths $ \term{left-successor-law-add}_\bN(m,n)$ and $\term{right-successor-law-add}_\bN(n,m)$ to obtain the identification we want. 
\end{proof}

\section*{September 22: Universes}

Recall that in Martin-L\"{o}f's dependent type theory, $\bN$ was defined as the inductive type freely generated by a term $0_\bN : \bN$ and a function $\suc :\bN \to \bN$. The corresponding induction principle gives a strengthened version of the Dedekind-Peano principle of mathematical induction, but two of the traditional axioms---namely that $0_\bN$ is not a successor and $\suc$ is injective---are missing. Using our type forming operations, we can define the types that assert those axioms:
\[ \Pi_{n : \bN} (n = 0_\bN) \to \emptyset \qquad \Pi_{n,m : \bN} (\suc(n) = \suc(m)) \to (n = m)\]
but we don't yet have the tools needed to construct terms in those types. Type theoretic \emph{universes} will enable us to construct terms in these types and prove many other things besides.

Informally, a universe $\UU$ can be thought of as a ``type whose terms are types.'' More precisely, a universe is a type $\UU$ together with a type family $X : \UU \vdash \sT(X)$ called the \emph{universal type family}. We think of the term $X$ as an \emph{encoding} of the type $\sT(X)$ though its common to conflate these notions notationally, writing ``$X$'' for both the encoding and the type.

Universes are assumed to be closed under all the type constructors in a sense to be made precise below. To avoid a famous inconsistency, however, we do not assume that the universe is contained in itself. One way to think about this is that $\UU$ is the type of ``small'' types, but $\UU$ itself is not ``small.''

In the presence of a universe $\UU$, a family of small types $x : A \vdash B(x) \univ$ over a type $A$ can be encoded by a function $B \colon A \to \UU$ defined by sending the term $x$ to the encoding of the type $B(x)$.\footnote{This is already how we have been defining type families in \texttt{agda}.} In particular, if $A$ is an inductive type, freely generated by some finite list of constructors, then \emph{type families} over $A$---not just dependent functions over $A$---can be defined inductively by specifying types for each of the constructors. We will see examples of this soon.

\subsection*{Type theoretic universes}

\begin{defn} A \textbf{universe} is a type $\UU$ in the empty context equipped with a type family $X : \UU \vdash \sT(X) \univ$ over $\UU$ called the \textbf{universal family of types} that is closed under the type forming operations in the sense that it is equipped with the following structure:
\begin{enumerate}
\item $\UU$ contains terms $\check{\emptyset}$, $\check{\1}$, $\check{\bN}$ that satisfy the judgmental equalities
\[ \sT(\check{\emptyset}) \doteq \emptyset, \quad \sT(\check{\1}) \doteq \1, \quad \sT(\check{\bN}) \doteq \bN.\]
\item $\UU$ is closed under coproducts in the sense that it comes equipped with a function
\[ \check{+} \colon \UU \to \UU \to \UU\] that satisfies $\sT(X\check{+}Y) \doteq \sT(X) + \sT(Y)$.
\item $\UU$ is closed under $\Pi$-types in the sense that it comes equipped with a function
\[ \check{\Pi} \colon \Pi_{X: \UU} (\sT(X) \to \UU) \to \UU\] satisfying
\[ \sT(\check{\Pi}(X,P)) \doteq \Pi_{x : \sT(X)} \sT(P(x))\]
for all $X : \UU$ and $P \colon \sT(X) \to \UU$.
\item $\UU$ is closed under $\Sigma$-types in the sense that it comes equipped with a function
\[ \check{\Sigma} \colon \Pi_{X: \UU} (\sT(X) \to \UU) \to \UU\] satisfying
\[ \sT(\check{\Sigma}(X,P)) \doteq \Sigma_{x : \sT(X)} \sT(P(x))\]
for all $X : \UU$ and $P \colon \sT(X) \to \UU$.
\item $\UU$ is closed under identity types in the sense that it comes equipped with a function
\[ \check{\Id} : \Pi_{X : \UU} \sT(X) \to \sT(X) \to \UU\]
satisfying
\[ \sT(\Id(X,x,y)) \doteq (x = y)\]
for all $X: \UU$ and $x,y : \sT(X)$.
\end{enumerate}
\end{defn} 

\begin{defn} Given a universe $\UU$, we say a type $A$ in context $\Gamma$ is \textbf{small} if it occurs in the universe: i.e., if it comes equipped with a term $\check{A} : \UU$ in context $\Gamma$ for which the judgment
\[ \Gamma \vdash \sT(\check{A}) \doteq A \univ\]
holds. 
\end{defn}

When $A$ is a small type, it's common to write $A$ for both $\check{A}$ and $\sT(A)$. So by $A : \UU$ we mean that $A$ is a small type.

\subsection*{Assuming enough universes}

Most of the time it's sufficient to assume just one universe $\UU$. But on occasion, it is useful to assume that $\UU$ itself is a type in some universe. 

\begin{post} We assume that there are \textbf{enough universes}, i.e., that for every finite list of types in context
\[ \Gamma_1 \vdash A_1 \univ \quad \cdots \quad \Gamma_n \vdash A_n \univ\]
there is a universe $\UU$ that contains each $A_i$ in the sense that $\UU$ has terms
\[ \Gamma_i \vdash \check{A}_i : \UU\] for which $\Gamma_i \vdash \sT(\check{A}_i) \doteq A_i \univ$ holds. 
\end{post}

With this assumption it's rarely necessary to work with more than one universe at the same time. 

As a consequence of our postulate that there exist enough universes, we obtain specific universes:\footnote{In \texttt{agda}, this structure is formalized in the file Agda.Primitive.}

\begin{defn} The \textbf{base universe} $\UU_0$ is obtained by applying the postulate to the empty list of types in context.
\end{defn}

\begin{defn} The \textbf{successor universe} of any universe $\UU$ is the universe $\UU^+$ obtained from the finite list
\[ \vdash \UU \univ \quad X: \UU \vdash \sT(X) \univ\]
\end{defn}
Thus the successor universe contains both $\UU$ and any type in $\UU$. 

\begin{defn} The \textbf{join} of two universes $\UU$ and $\mathcal{V}$ is the universe $\UU \sqcup \mathcal{V}$ obtained by applying the postulate to the type families
\[ X : \UU \vdash \sT_\UU(X) \univ \qquad Y : \mathcal{V} \vdash \sT_{\mathcal{V}}(Y) \univ\]
\end{defn}

\subsection*{Observational equality on \texorpdfstring{$\bN$}{N}}

To illustrate what universes are for, we define a type family $m : \bN, n : \bN \vdash \Eq_\bN(m,n) \univ$ that we call \textbf{observational equality} on $\bN$. Because type families can now be thought of as functions $\Eq_\bN : \bN \to \bN \to \UU$ we can use the induction principle of $\bN$ to define this type family.  We'll then prove that $\Eq_\bN$ is \textbf{logically equivalent} to the identity type family; in fact, we'll later see that these types are \textbf{equivalent}, once we know what that means. The advantage of the type family $\Eq_\bN$ is that it's characterized more explicitly, so this will help us prove theorems about the identity type family over the natural numbers.

\begin{defn} We define \textbf{observational equality} of $\bN$ as the type family $\Eq_\bN : \bN \to \bN \to \UU$ satisfying
\[ \Eq_\bN(0_\bN,0_\bN) \doteq \1 \quad \Eq_\bN(\suc(n),0_\bN) \doteq \emptyset \quad \Eq_\bN(0,\suc(n)) \doteq \emptyset \quad \Eq_\bN(\suc(m), \suc(n)) \doteq \Eq_\bN(m,n).\]
\end{defn}

\begin{lem} Observational equality on $\bN$ is reflexive:
\[ \refl-\Eq_\bN : \Pi_{n : \bN} \Eq_\bN(n,n).\]
\end{lem}
\begin{proof}
We define $\refl-\Eq_\bN$ by induction by $\refl-\Eq_\bN(0_\bN) \coloneq \star$ and $\refl-\Eq_\bN(\suc(n)) \coloneq \refl-\Eq_\bN(n)$.
\end{proof}

\begin{prop} For any $m,n : \bN$, the types $\Eq_\bN(m,n)$ and $(m=n)$ are \textbf{logically equivalent}: that is there are functions
\[ (m = n) \to \Eq_\bN(m,n) \quad \text{and} \quad \Eq_\bN(m,n) \to (m=n).\]
\end{prop}
\begin{proof}
By path induction, there is a function $\term{id-to-eq} : \Pi_{m,n: \bN} (m =n) \to \Eq_\bN(m,n)$ defined by $\term{id-to-eq}(n,\refl_n) \coloneq \refl-\Eq_\bN(n)$. 

For the converse, we define a function $\term{eq-to-id} : \Pi_{m,n : \bN} \Eq_\bN(m,n) \to (m=n)$ by induction on $m$ and $n$. We define $\term{eq-to-id}(0_\bN,0_\bN) : \Eq_\bN(0_\bN,0_\bN) \to (0_\bN = 0_\bN)$, by induction on $\Eq_\bN(0_\bN,0_\bN) \doteq \1$ to be the function that sends $\star : \1$ to $\refl_{0_\bN} : 0_\bN = 0_\bN$. We define the functions $\term{eq-to-id}(\suc(n),0_\bN)$ and $\term{eq-to-id}(0_\bN,\suc(n))$ using $\term{ex-falso}$, since both of these are maps out of the empty type. Finally, to define $\term{eq-to-id}(\suc(m),\suc(n))$ we may use a function $f : \Eq_\bN(m,n) \to (m=n)$, in which case, $\term{eq-to-id}(\suc(m),\suc(n))$  is defined to be the composite function
\[ \Eq_\bN(\suc(m),\suc(n)) \xrightarrow{\id} \Eq(m,n) \xrightarrow{f} (m=n) \xrightarrow{\ap_{\suc}} (\suc(m)=\suc(n)).\]
\end{proof}

\begin{ntn} For types $A$ and $B$, we write $A \leftrightarrow B$ as an abbreviation for the type
\[ (A \to B) \times (B \to A).\]
\end{ntn}

Thus the logical equivalence defines a term in the type
\[ \Pi_{m,n : \bN} \Eq_\bN(m,n) \leftrightarrow (m=n).\]

\subsection*{Peano's axioms}

\begin{thm} For any $m,n : \bN$ we have
\[ (m=n) \leftrightarrow (\suc(m) = \suc(n))\]
\end{thm}
\begin{proof}
The action of paths of the successor function proves the forwards implication
\[ \ap_{\suc} : (m=n) \to (\suc(m) = \suc(n))\]
The direction of interest is the converse which proves that successor is injective.

Using the logical equivalences $(m=n) \leftrightarrow \Eq_\bN(m,n)$ we define the reverse implication to be the composite
\[ (\suc(m) = \suc(n)) \xrightarrow{\term{id-to-eq}(\suc(m),\suc(n))} 
\Eq_\bN(\suc(m),\suc(n)) \xrightarrow{\id} \Eq_\bN(m,n) \xrightarrow{\term{eq-to-id}(m,n)} (m=n).\]
\end{proof}

\begin{thm} For any $n : \bN$, $\neg(0_\bN  = \succ_\bN(n))$.
\end{thm}
\begin{proof}
We have a family of maps
\[ \lambda n, \term{id-to-eq}(0_\bN,n) : \Pi_{n : \bN} (0_\bN = n) \to \Eq_\bN(0_\bN,n).\]
Since $\Eq_\bN(0_\bN, \suc(n)) \doteq \emptyset$ we have
\[ \term{id-to-eq}(0_\bN,\suc(n)) : (0_\bN = \suc(n)) \to \emptyset\]
which is precisely the claim.
\end{proof}

\section*{September 27: Modular arithmetic}

Having fully described Martin-L\"{o}f's dependent type theory, we may now start developing some mathematics in it. The fundamental idea used to develop mathematics is something we've already previewed: the Curry-Howard interpretation.

\subsection*{The Curry-Howard interpretation}

The Curry-Howard interpretation is an interpretation of logic into type theory. In type theory, there is no separation between the logical framework and the general theory of collections of mathematical objects the way there is in the more traditional setup with Zermelo-Fraenkel set theory, which is postulated by axioms in first order logic. The idea is that propositions may be expressed as types with proofs of those propositions expressed as terms in those types. For example:

\begin{defn} We say that a natural number $d$ divides a natural number $n$ if there is a term in the type
\[ d \mid n \coloneq \Sigma_{k : \bN} d \cdot k = n \]
defined using the multiplication $\cdot$ on $\bN$, the identity type of $\bN$, and the dependent sum of the type family $k : \bN \vdash d \cdot k = n \univ$.
\end{defn}

Just as existential quantification $(\exists)$ is expressed using $\Sigma$-types, universal quantification $(\forall)$ is expressed using $\Pi$-types. For example, the type
\[ \Pi_{n : \bN} 1 \mid n\]
asserts that every natural number is divisible by 1. The term
\[ \lambda n. (n, \term{left-unit}(n)) : \Pi_{n : \bN} 1 \mid n\]
proves this result.

\begin{prop} Let $d, m,n : \bN$. If $d$ divides any two of $m$, $n$, and $m+n$, then $d$ divides the third.
\end{prop}
\begin{proof}
We prove only that if $d \mid m$ and $d \mid n$ then $d \mid m+n$.
By hypothesis we have terms:
\[ H : \Sigma_{k : \bN} d \cdot k = m \quad \text{and} \quad K : \Sigma_{k : \bN} d \cdot k = n.\]
By $\Sigma$-induction, we may assume that $H$ is given by a pair $(h : \bN, p : d \cdot h = m)$ and $K$ is given by a pair $(k : \bN, q : d \cdot k = n)$. To get a term in $\Sigma_{x : \bN} d \cdot x = m + n$ we may use $x \coloneq h + k$. Our goal is then to define an identification $d \cdot (h + k) = m + n$ which we obtain as a concatenation
\[
\begin{tikzcd} d \cdot (h+ k ) \arrow[r, equals, "\term{dist}"] & d \cdot h + d \cdot k \arrow[r, equals, "\ap_{+d \cdot k}p"] & m + d \cdot k \arrow[r, equals, "\ap_{m+}q"] & m + n
\end{tikzcd}
\]
\end{proof}

We have observed many similarities between the rules of various type constructors and tautologies from logic. For instance, the elimination rule for the non-dependent function type supplies a function
\[ \term{modus-ponens} : A \times (A \to B) \to B.\]
One important difference is that general types may contain multiple terms that cannot be identified: i.e., for which it is possible to prove that $x =_A y \to \emptyset$. Later we'll study the following predicate on types:
\[ \term{is-prop}(A) \coloneq \Pi_{x,y : A} x =_A y\]
which asserts that \emph{if} $A$ has multiple terms (which it may not) those terms can always be identified. This will be the $n=-1$ level of a hierarchy of $n$-types for $n \geq -2$.

\subsection*{The congruence relations on \texorpdfstring{$\bN$}{N}}

The family of identity types can be understood as a type-valued binary relation on a type.

\begin{defn} For a type $A$, a \textbf{typal binary relation} on $A$ is a family of types $x, y : A \vdash R(x,y) \univ$. A binary relation $R$ is 
\begin{itemize}
\item \textbf{reflexive} if it comes with a term $\rho : \Pi_{x :A} R(x,x)$,
\item \textbf{symmetric} if it comes with a term $\sigma : \Pi_{x,y :A} R(x,y) \to R(y,x)$,
\item \textbf{transitive} if it comes with a term $\tau : \Pi_{x,y,z : A}  R(x,y) \to R(y,z) \to R(x,z)$
\end{itemize}
A \textbf{typal equivalence relation} on $A$ is a reflexive, symmetric, and transitive, typal binary relation.
\end{defn}

For instance, for each $k : \bN$ we can define the relation of congruence modulo $k$ by defining a type
\[ x \equiv y \mod k\]
for each $x,y : \bN$ comprised of proofs that $x$ is equivalent to $y$ modulo $k$. Following Gauss, we say that $x$ is equivalent to $y$ mod $k$ if $k$ divides the symmetric difference $\term{dist}_\bN(x,y)$ defined recursively by
\[ \term{dist}_\bN(0,0) \coloneq 0 \quad \term{dist}_\bN(0, y+1) \coloneq y+1 \quad \term{dist}_\bN(x+1,0) \coloneq x+1, \quad \term{dist}_\bN(x+1,y+1) \coloneq \term{dist}_\bN(x,y).\]

\begin{defn} For $k,x,y : \bN$ define
\[ x \equiv y \mod k \coloneq k \mid \term{dist}_\bN(x,y).\]
Note this defines the \emph{type} $x \equiv y \mod k$. A term is then a pair comprised of an $\ell : \bN$ together with an identification $k \cdot \ell = \term{dist}_\bN(x,y)$.
\end{defn}

We leave the following to the course text:

\begin{prop} For each $k$, the typal relation $\equiv \mod k$ is an equivalence relation.
\end{prop}

There are other important relations on $\bN$ that are not-equivalence relations.

\begin{defn} The binary relation $\leq$ on $\bN$ is defined by induction by
  \[ 0 \leq 0 \coloneq \1 \quad 0 \leq n+1 \coloneq \1 \quad n+1 \leq 0 \coloneq \emptyset \quad m+1 \leq n+1 \coloneq m \leq n.\]
  Similarly, the binary relation $<$ is defined by
  \[ 0 < 0 \coloneq \emptyset \quad 0 < n+1 \coloneq \1 \quad n+1 < 0 \coloneq \emptyset \quad m+1 < n+1 \coloneq m < n.\]
  \end{defn}

\subsection*{The standard finite types}

The standard finite sets are classically defined as the sets $\{ n \in \bN \mid n < k\}$, so how do we interpret a subset $\{ x  \in A \mid P(x)\}$ characterized by a predicate in type theory?

In the Curry-Howard interpretation, the predicate $P(x)$ is interpreted as a type family and the type of terms $x$ in $A$ for which $P(x)$ is true is interpreted by the $\Sigma$-type $\Sigma_{x : A} P(x)$. Note for a general type family $P(x)$ it won't necessarily be the case that the map $\pr_1 \colon \Sigma_{x :A} P(x) \to A$ is a monomorphism\footnote{Though this will be the case if each type $P(x)$ is a proposition in the sense alluded to above.} so this construction operates a bit differently than in set theory.

Through this mechanism it is possible to define the classical finite sets as 
\[ \type{Classical-Fin}_k \coloneq \Sigma_{n : \bN}n < k \]
though the standard definition is as follows:

\begin{defn} We define the type family $\type{Fin}$ of \textbf{standard finite types} inductively (using the induction principle of $\bN$ and the universe $\UU$) as follows:
\[ \type{Fin}_0 \coloneq \emptyset, \quad \type{Fin}_{k+1} \coloneq \type{Fin}_k + \1.\]
\end{defn}

Write $i$ for $\inl : \type{Fin}_k \to \type{Fin}_{k+1}$ and $\star$ for the point $\inr(\star) : \type{Fin}_{k+1}$.

By induction we can define functions $\iota_k \colon \type{Fin}_k \to \bN$ for each $k$. When $k=0$ there is nothing to show. To define $\iota_{k+1} \colon \type{Fin}_{k+1} \to \bN$ we can use $\iota_k$  and define $\iota_{k+1}(i(x)) \coloneq \iota_k(x)$ and $\iota_{k+1}(\star)\coloneq k$.

\subsection*{The natural numbers modulo $k+1$}

Given an equivalence relation $\sim$ on a set $A$ the quotient $A_{/\sim}$ comes equipped with a quotient map $q \colon A \to A_{/\sim}$ that satisfies two important properties:
\begin{enumerate}
\item $q$ is \textbf{effective}: $q(x) = q(y)$ if and only if $x \sim y$
\item $q$ is surjective: for all $[z] \in A_{/\sim}$ there is some $z \in A$ so that $q(z) = [z]$.
\end{enumerate}

Both properties can be expressed in type theory, though there are some subtleties.

\begin{defn} In the context of types $A$ and $B$ there is a type family $\type{is-surj} \colon (A \to B) \to \UU$ defined by
\[ \type{is-surj}(f) \coloneq \Pi_{b : B} \Sigma_{a : A} f(a) =_B b.\]
\end{defn}

The subtlety is that this really defines a \emph{split} notion of surjectivity. A term $p$ in $\type{is-surj}(f)$ defines a function that for each term $b : B$ produces a term of $\Sigma_{a : A} f(a) =_B b$. By compositing $p$ with $\pr_1 : \Sigma_{a : A} f(a) =_B b \to A$, we obtain a function $s \colon B \to A$. By composing $p$ with $\pr_2 : \Sigma_{a : A} f(s(b)) =_B b$ we also obtain a proof that $s$ is a \textbf{section} of $f$. Thus surjective functions in homotopy type theory are really \textbf{split} surjective functions.

Our next challenge is to define the quotient maps $[-]_{k} \colon \bN \to \type{Fin}_{k}$ that compute the remainder modulo $k$. Our strategy will be to define this function by induction on $n : \bN$. The idea is that the term $0_\bN : \bN$ should get sent to some $0$ while successors in $\bN$ should be sent to successors in $\type{Fin}_k$, taken in the cyclic order. We define these auxiliary structures first.

\begin{defn} We define $\term{zero}_k : \type{Fin}_{k+1}$ recursively by
\[ \term{zero}_0 \coloneq \star \quad \term{zero}_{k+1} \coloneq i(\term{zero}_k).\]

We then define $\term{skip-zero}_k : \type{Fin}_k \to \type{Fin}_{k+1}$ recursively by
\[ \term{skip-zero}_{k+1}(i(x)) \coloneq i(\term{skip-zero}_k(x)) \quad \term{skip-zero}_{k+1}(\star) \coloneq \star.\]

Finally, we define $\term{succ}_k : \type{Fin}_k \to \type{Fin}_k$ recursively by
\[ \term{succ}_{k+1}(i(x)) \coloneq \term{skip-zero}_k(x) \qquad \term{succ}_{k+1}(\star) \coloneq \term{zero}_k.\]
\end{defn}


\begin{defn} For any $k : \bN$ define $[-]_{k+1} \colon \bN \to \type{Fin}_{k+1}$ by
\[ [0]_{k+1} \coloneq 0 \quad \text{and} \quad [n+1]_{k+1} \coloneq \term{succ}_{k+1}[n]_{k+1}.\]
\end{defn}

The text goes on to show that 
\begin{itemize}
\item $n \equiv \iota [n]_{k} \mod k$ for all $n$ and $k$,
\item $[n]_k = [m]_k$ if and only of $n \equiv m \mod k$,
\item and the map $[-]_k \colon \bN \to \type{Fin}_k$ is split surjective.
\end{itemize}

Then it is possible to use this quotient map to define the cyclic group structure on $\type{Fin}_k$.

\section*{September 29: Decidability in elementary number theory}

In constructive mathematics it is not possible to prove the law of excluded middle: namely that $P \vee \neg P$ for an arbitrary proposition $P$. Similarly in type theory, it is not possible to construct a term of type $A + \neg A$ for arbitrary $A$. But certain types do come with such terms.

\subsection*{Decidability}

\begin{defn} A type $A$ is \textbf{decidable} if it comes equipped with an element of type
\[ \type{is-decidable}(A) \coloneq A + \neg A.\]
A type family $P \colon A \to \UU$ is \textbf{decidable} if $P(a)$ is decidable for every $a : A$.
\end{defn}

\begin{ex}
The primary way to show that $A$ is decidable is either to provide a term $a : A$ or provide a function $na : A \to \emptyset$. In particular $\1$ is decidable since we have $\inl(\star) : \type{is-decidable}(\1)$. Similarly $\emptyset$ is decidable since we have $\inr(\id) : \type{is-decidable}(\emptyset)$.
\end{ex}

\begin{ex} Since the type families $n, m: \bN \vdash n \leq m \univ$ and $n, m : \bN \vdash n < m \univ$ were defined by induction from the types $\emptyset$ and $\1$, it follows that these type families are decidable.
\end{ex}

\begin{rmk}
If $A$ and $B$ are decidable then so are $A+B$, $A \times B$, and $A \to B$. Proofs use case analysis over the coproduct types $A + \neg A$ and $B + \neg B$. 
\end{rmk}

\begin{defn} A type $A$ \textbf{has decidable equality} if the identity type $x=_Ay$ is decidable for every $x, y : A$. Thus
\[ \type{has-decidable-eq}(A) \coloneq \Pi_{x,y: A} \type{is-decidable}(x=_Ay).\]
\end{defn}

\begin{lem} Suppose $A$ and $B$ are types so that $A \leftrightarrow B$. Then $A$ is decidable if and only if $B$ is decidable.
\end{lem}
\begin{proof}
A proof of $A \leftrightarrow B$ supplies functions $f \colon A \to B$ and $g \colon B \to A$. Using the contrapositive function we obtain $\term{contrapositive}(f) : \neg B \to \neg A$ and $\term{contrapositive}(g) \colon \neg A \to \neg B$. We therefore have functions
\[ f + \term{contrapositive}(g) \colon A + \neg A \to B + \neg B \qquad g + \term{contrapositive}(f) : B + \neg B \to A + \neg A,\]
proving the logical equivalence of $\type{is-decidable}(A)$ and $\type{is-decidable}(B)$. In particular, if either type is inhabited, both must be. 
\end{proof}

\begin{cor} $\bN$ has decidable equality.
\end{cor}
\begin{proof}
We have shown that the identity types of $\bN$ are logically equivalent to the observational equality types, which were defined to be $\1$ or $\emptyset$. As both types are decidable, the identity types of $\bN$ must be as well.
\end{proof}

\begin{rmk} We will prove later that if a type has decidable equality then it must be a \textbf{set} in a technical sense to be introduced. Even so, not all sets have decidable equality unless one assumes that the law of excluded middle is true for all propositions.
\end{rmk}

\subsection*{Case analysis}

Suppose you'd like to define a function by case analysis such as $\term{collatz} : \bN \to \bN$
\[ \term{collatz}(n) \coloneq \begin{cases} n/2 & n\ \text{is~odd} \\ 2n+1 & n\ \text{is~even} \end{cases}\]
To justify this sort of case analysis we use a term
\[ d : \Pi_{n : \bN} \type{is-decidable} (2 \mid n)\]
whose construction we skipped. Note that $2 \mid n$ and $\neg(2 \mid n)$ cannot both hold because if so we could evaluate the function $\term{odd}(n) : \neg(2 \mid n)$ at the term $\term{even}(n) : 2 \mid n$ to get a contradiction. So this $d$ can be thought of as a proof that for all $n : \bN$, $n$ is odd or $n$ is even (but not both).

This puts us into the following abstract setup. Our goal is to define a function $c : \Pi_{x :A} C(x)$, namely the function $\term{collatz} : \bN \to \bN$. We already have a function $d : \Pi_{x :A} B(x)$, namely $d : \Pi_{n : \bN} \type{is-decidable}(2 \mid n)$. So it suffices to define a function $h : \Pi_{x:A} B(x) \to C(x)$ because then we can define $c(x) \coloneq h(x,d(x))$. In this case this means we need a function
\[ h : \Pi_{n : \bN} \type{is-decidable} (2 \mid n) \to \bN\] which we can now define by cases from
\[ h\term{-even}(n) \coloneq \lambda n. n/2 \colon (2 \mid n) \to \bN \quad \text{and} \quad h\term{-odd}(n) \coloneq \lambda n. 3n+1 : \neg(2 \mid n) \to \bN.\]
There is something called the ``with-abstraction'' that gives a concise syntax for functions defined in this manner.

\subsection*{The well-ordering principle of $\bN$}

The traditional well-ordering principle is about subsets of $\bN$, or equivalently, about predicates on $\bN$. In type theory, we replace these by decidable type families over $\bN$.

\begin{defn} Let $P \colon \bN \to \UU$. A number $n : \bN$ is a \textbf{lower bound} for $P$ if it comes equipped with a term in the type
  \[ \type{is-lower-bound}_{P}(n) \coloneq \Pi_{k : \bN} P(k) \to n \leq k\]
  Similarly,
  \[ \type{is-upper-bound}_{P}(n) \coloneq \Pi_{k : \bN} P(k) \to k \leq n\]
  \end{defn}

  \begin{thm}[well-ordering principle]
    Let $P$ be a decidable family over $\bN$ with $d$ a witness that $P$ is decidable. Then there is a function
    \[ w(P,d) : \left( \Sigma_{n : \bN}P(n)\right) \to \left( \Sigma_{m : \bN}P(m) \times \type{is-lower-bound}_P(m)\right).\]
  \end{thm}
  In other words, if $P(n)$ is inhabited for some $n$ then there is a smallest $m : \bN$ so that $P(m)$ is inhabited.

  \begin{proof}
    We will show that for any decidable type family $Q : \bN \to \UU$ that there is a function
    \[ Q(n) \to \Sigma_{m : \bN}Q(m) \times \type{is-lower-bound}_Q(m)
    \]
    by induction on $n$. When $n=0$ we can use $m=0$ since 0 is always a lower bound. For the inductive step we may assume we have the displayed function for every type family $Q$ and consider a decidable type family $Q$ with a term $q : Q(n+1)$. Our goal is to construct a term in the type
    \[ \Sigma_{m : \bN}Q(m) \times \type{is-lower-bound}_{Q}(m)\]. Since $Q(0)$ is decidable it suffices to construct a function
    \[ Q(0) + \neg Q(0) \to  \Sigma_{m : \bN}Q(m) \times \type{is-lower-bound}_{Q}m
    \]
    so we can do a case analysis. If we have $Q(0)$ then it follows immediately that $m=0$ is minimal.  If $\neq Q(0)$, then we can consider the decidable family $Q' \colon \bN \to \UU$ defined by $Q(n) \coloneq Q'(\suc(n))$. Since $q : Q'(n)$ we get a minimal element $m$ for $Q'$ by the inductive hypothesis. But since $Q(0)$ is assumed to be false then $m+1$ is the minimal element for $Q$.   
    \end{proof}
    
  

\subsection*{The infinitude of primes}  

For natural numbers $d$ and $n$ we say $d$ is a \textbf{proper divisor} of $n$ if it comes with a term in the type
\[ \type{is-proper-divisor}(n,d) \coloneq (d \neq n) \times (d \mid n)\]
With this notation we can say a natural number $n$ is \textbf{prime} if it comes with a term in the type
\[ \type{is-prime}(n) \coloneq \Pi_{x : \bN}\type{is-proper-divisor}(n,x) \leftrightarrow (x=1)\]

The proof of the infinitude of primes proceeds by constructing a prime number larger than $n$ for any $n : \bN$. So we can consider the type family for $n,m : \bN$
\[ R(n,m) \coloneq (n < m) \times \Pi_{x : \bN} (x \leq n) \to (x \mid m) \to (x=1)\]
of pairs so that $m$ is greater than $n$ and $m$ is relatively prime to all numbers $x \leq n$. Since $n!+1$ satisfies these properties for any $n$, the type family $m \mapsto R(n,m)$ in context $n : \bN$ is inhabited. Thus, by the well-ordering principle, it has a least element $p$ and this $p$ must be prime.

Using the results we skipped it's possible to prove:

\begin{lem} The type $R(n,m)$ is decidable for each $n, m : \bN$.
\end{lem}

We leave it to the reader to verify that $R(n,n!+1)$ is inhabited. Using these ingredients, we prove the infinitude of primes in the following form:

\begin{thm} For each $n$, there is a prime number $p : \bN$ so that $n < p$.
  \end{thm}
  \begin{proof}
    It suffices to show this for each non-zero $n$ since the case $n=0$ follows. So let $n$ be a non-zero natural number.

    Since the type $R(n,m)$ is decidable for each $m$ and since $R(n,n!+1)$ holds by the well-ordering principle there is a minimal $p : \bN$ so that $R(n,p)$. We will show that $p$ is prime by constructing a term in the type
    \[ \type{is-prime}(p) \coloneq (p \neq 1) \times \Pi_{x: \bN} \type{is-proper-divisor}(p,x) \to (x = 1).\]
    By construction, $n < p$ and $n$ is non-zero so $p \neq 1$. So now let $x$ be a proper divisor of $p$. Since $R(n,p)$ holds by construction we can show that $x=1$ by proving that $x \leq n$. Since $p$ is non-zero and $x \mid p$ we must have $x < p$. By minimality of $p$ it follows that $\neg R(n,x)$ holds. However, any divisor of $x$ must also divide $p$ by transitivity of divisibility, so \[
      \Pi_{y : \bN} (y \leq n) \to (y \mid x) \to (y=1).\]
    Since \[\neg R(n,x) \doteq \neg \left((n < x) \times \Pi_{y : \bN} (y \leq n) \to (y \mid x) \to (y=1) \right)\]
         holds we conclude that $\neg(n < x)$. On account of the logical equivalence $\neg(n < x ) \leftrightarrow (x \leq n)$, it follows that $x \leq n$.
  \end{proof}
 
  
\part{The Univalent Foundations of Mathematics}

The univalent foundations build on Martin-L\"{o}f's dependent type theory by adding a few new axioms inspired by the interpretation of types as $\infty$-groupoids or spaces rather than sets. One of these axioms, Voevodsky's \emph{univalence axiom}, is inconsistent with the interpretation of types as sets. When we meet the hierarchy of truncation levels, we'll see that some types behave like types and some types behave like sets while others have non-trivial higher-dimensional structures. 

\section*{October 4: Equivalences}

Recall the notion of \textbf{logical equivalence} between types $A$ and $B$
\[ A \leftrightarrow B \coloneq (A \to B) \times (B \to A).\]

For instance, you proved on your homework that $\bool$ and $\1+\1$ are logically equivalent by constructing functions
\[
\begin{tikzcd}[row sep=tiny, column sep=large] \bool \arrow[r, "{\term{bool-to-1+1}}"] & \1+\1 & \1+\1 \arrow[r, "\term{1+1-to-bool}"] & \bool \\
\true \arrow[r, mapsto] & \inl(\star) & \inl(\star) \arrow[r, mapsto] & \true \\ \false \arrow[r, mapsto] & \inr(\star) & \inr(\star) \arrow[r, mapsto] & \bool
\end{tikzcd}
\]
These inverse implications are obviously related. Our next goal is to develop language to explain how.

\subsection*{Homotopies}

Surprisingly, we are not able, in dependent type theory, to construct identifications between functions $\term{1+1-to-bool} \circ \term{bool-to-1+1}$ and $\id_\bool$ or between $\suc$ and $\lambda n, n+1$. We can, however, construct \emph{pointwise identifications} between these pairs of functions, which in homotopy type theory are commonly called \textbf{homotopies}:

\begin{defn} Let $f,g : \Pi_{x:A} B(x)$. The type $f\sim g$ of \textbf{homotopies} from $f$ to $g$ is defined to be the type of pointwise identifications:
\[ f \sim g\coloneq \Pi_{x :A} f(x) =_{B(x)} g(x).\]
\end{defn}

\begin{rmk} Given three functions as below
\[
\begin{tikzcd} A \arrow[rr, "h"] \arrow[dr, "f"'] & & C \\ & B \arrow[ur, "g"']
\end{tikzcd}
\]
we say the triangle \textbf{commutes} if $h\sim g \circ f$.
\end{rmk}

Note if $H, K : f \sim g \coloneq \Pi_{x :A} f(x) = g(x)$ are homotopies we can treat them as dependent functions in their own right and consider \textbf{homotopies between homotopies}, i.e., terms of the type $H \sim K \coloneq \Pi_{x :A} H(x) = K(x)$. This continues all the way up.

Indeed, the $\infty$-groupoid structure of identity types extends to homotopies as follows:

\begin{defn} For any type family $B \colon A \to \UU$ we have operations
\[ \term{refl-htpy} : \Pi_{f : \Pi_{x:A}B(x)} f \sim f \quad \term{inv-htpy} : \Pi_{f,g :\Pi_{x:A}B(x)} (f \sim g) \to (g \sim f) \]
\[ \term{concat-htpy} : \Pi_{f,g,h : \Pi_{x:A}B(x)} f \sim g \to g \sim h \to f \sim h\]
defined pointwise by
\[ \term{refl-htpy}(f) = \lambda x .\refl_{f(x)} \quad \term{inv-htpy}(H) \coloneq \lambda x. {H(x)}^{-1} \quad \term{concat-htpy}(H,K) \coloneq \lambda x. H(x) \cdot K(x)\]
We abbreviate these latter two terms by writing $H^{-1}$ and $H \cdot K$.
\end{defn}

Note we don't abbreviate $\term{refl-htpy}_f : f \sim f \coloneq \Pi_{x :A} f(x)=_{B(x)} f(x)$ as $\refl_f : f =_{\Pi_{x:A}B(x)}f$ as these terms live in different types.

\begin{prop} Homotopies satisfy the groupoid laws up to homotopy:
\begin{enumerate}
\item There is a homotopy $\term{assoc-htpy}(H,K,L) : (H \cdot K) \cdot L \sim H \cdot (K \cdot L)$ for any homotopies $H : f \sim g$, $K : g \sim h$, $L \colon h \sim i$.
\item There are homotopies $\term{left-unit-htpy}(H) : \term{refl-htpy}_f \cdot H \sim H$ and $\term{right-unit-htpy}(H) : H \cdot \term{refl-htpy}_g \sim H$.
\item There are homotopies $\term{left-inv-htpy}(H) : H^{-1} \cdot H \sim \term{refl-htpy}_g$ and $\term{right-inv-htpy}(H) \colon H \cdot H^{-1} \sim \term{refl-htpy}_f$.
\end{enumerate}
\end{prop}

In addition to the groupoid operations we make use of the following \textbf{whiskering operations} on homotopies which are relevant in the setting of functions
\[
\begin{tikzcd} A \arrow[r, "f"] & B \arrow[r, bend left, "g"] \arrow[r, bend right, "h"'] & C \arrow[r, "k"] & D
\end{tikzcd}
\]

\begin{defn} Given the functions above and a homotopy $H \colon g \sim h$ define homopies
\[ H \circ f \coloneq \lambda a. H(f(a)) \colon g \circ f \sim h \circ f\]
and
\[
k \circ H \coloneq \lambda_b. \ap_k(H(b)) \colon k \circ g \sim k \circ h.\]
\end{defn}

\subsection*{Bi-invertible maps}

We now explain what it means for a function $f \colon A \to B$ between types to define an \textbf{equivalence}. One explanation is that an equivalence is like a homotopy equivalence in topology, but we actually define the type family $\type{is-equiv}$ in a manner that makes equivalences look more like ``bi-invertible maps'' for reasons that we will explain.

\begin{defn} Let $f \colon A \to B$. 
\begin{enumerate}
\item The type of \textbf{sections} of $f$ is defined to be the type
\[ \type{sec}(f) \coloneq \Sigma_{g : B \to A} f\circ g \sim \id_B.\]
\item The type of \textbf{retractions} of $f$ is defined to be the type
\[ \type{retr}(f) \coloneq \Sigma_{h : B \to A} h \circ f \sim \id_A.\]
\item We say $f$ is an \textbf{equivalence} if it has a section and a retraction:
\[ \type{is-equiv}(f) \colon \type{sec}(f) \times \type{retr}(f).\]
\end{enumerate}
We write $A \simeq B$ for the type $\Sigma_{f : A \to B} \type{is-equiv}(f)$ of all equivalences from $A$ to $B$.
\end{defn}

Explicitly the data of a term in $\type{is-equiv}(f)$ involves two maps $g, h : B \to A$ and two homotopies $G : f \circ g \sim \id_B$ and $H \colon h \circ f \sim \id_A$. We have seen a bunch of examples:

\begin{ex} $\quad$
\begin{itemize}
\item Identity functions are equivalences.
\item $\term{bool-to-1+1} \colon \bool \to \1+\1$ and $\term{1+1-to-bool} \colon \1+\1 \to \bool$ are inverse equivalences.
\item $\term{neg-bool} \colon \bool \to \bool$ is an equivalence.
\item $\term{pred} \colon \bZ \to \bZ$ is an equivalence, inverse to $\term{succ} \colon \bZ \to \bZ$.
\end{itemize}
\end{ex}

We might also define
\[ \type{has-inverse}(f) \coloneq \Sigma_{g : B \to A} (f \circ g \sim \id_B) \times (g \circ f \sim \id_A).\]
The reason we did not define equivalences to be functions that have inverses is that we wanted being an equivalence to be a \textbf{property} associated to a map $f$ rather than a \textbf{structure} on the map, in a sense we'll discuss soon. Essentially because of the uniqueness of reflexivity paths, if $f$ is an equivalence then it only has one section and homotopy and only one retraction and homotopy, up to homotopy. We'll see, however, that the data of inverses to $f$ can be more complicated.

But even though the data encoded by terms in $\type{is-equiv}(f)$ and $\type{has-inverse}(f)$ may differ, these types are closely related:

\begin{prop} The types $\type{is-equiv}(f)$ and $\type{has-inverse}(f)$ are logically equivalent. In particular, if $f$ is an equivalence it has a two-sided inverse.
\end{prop}
\begin{proof}
The data of an inverse obviously defines the data of an equivalence, so we have $\type{has-inverse}(f) \to \type{is-equiv}(f)$.

For the converse, suppose we have $g,h \colon B \to A$ and homotopies $G : f \circ g \sim \id_B$ and $H \colon h \circ f \sim \id_A$.  Then for any $b : B$ we have
\[
\begin{tikzcd} g(b) \arrow[r, equals, "H(g(b))^{-1}"] & h(f(g(b))) \arrow[r, equals, "\ap_h(G(b))"] & h(b)
\end{tikzcd}
\]
defining a homotopy $K \colon g \sim h$. Using this we can see that $g$ is also a retraction of $f$ by the identification for any $a :A$
\[
\begin{tikzcd} g(f(a)) \arrow[r, "K(f(a))", equals] & h(f(a)) \arrow[r, "H(a)"] & a
\end{tikzcd}
\]
\end{proof}

\begin{cor} A section or retraction of an equivalence is an equivalence. 
\end{cor}
\begin{proof}
We've just seen that the section of an equivalence is also a retraction and thus is an invertible map with inverse $f$. A dual construction applies to a retraction.
\end{proof}

Up to equivalence, types satisfy many familiar laws, such as
\begin{align*}
A+B &\simeq B + A & A \times B &\simeq B \times A\\
A \times \1 &\simeq A & A+\emptyset &\simeq A \\
A \times (B + C) &\simeq (A \times B) + (A \times C)
 \end{align*}
where the required equivalences and homotopies can be constructed by induction. These equivalences extend to cases such as
\[ \Sigma_{x : \emptyset} B(x) \simeq \emptyset \quad \Sigma_{x : \1}B(x)\simeq B(\star).\]
and
\[ \Sigma_{z : \Sigma_{x : A}B(x)} C(z) \simeq \Sigma_{x :A} \Sigma_{y : B(x)} C(x,y) \quad \Sigma_{w : A +B}C(w) \simeq \Sigma_{x :A} C(\inl x) + \Sigma_{y : B} C(\inr y).\]

In the presence of an additional axiom, we'll be able to prove similar equivalences involving function types, but we can't deduce these just yet.

\subsection*{Characterizing the identity types of \texorpdfstring{$\Sigma$}{Sigma}-types}

We return to the theme of characterizing identity types to characterize the identity type of a $\Sigma$-type, up to equivalence. We follow the same outline used to characterize the identity type family of the natural numbers above:
\begin{enumerate}
\item We define a binary relation $R \colon A \to A \to \UU$ that we suspect is equivalent to the identity type family.
\item Prove reflexivity by constructing a term in $\Pi_{x :A} R(x,x)$.
\item Use reflexivity and path induction to define a canonical map $\Pi_{x,y:A} x=_A y \to R(x,y)$.
\item Show that the map $x=_A y \to R(x,y)$ is an equivalence for each $x,y :A$.
\end{enumerate}
The last step is usually the most difficult and we will refine our techniques for dealing with it soon.

In this section, we consider $B \colon A \to \UU$ and the corresponding type $\Sigma_{x:A} B(x)$. Given dependent pairs $(a,b)$ and $(a',b')$ we might imagine that the data of an identification between them involves a pair of identifications comprised firstly of a path $p : a =_A a'$ and then of a path $q : \tr_B(p, b) = b'$. We first turn this idea into a binary relation.

\begin{defn} Define 
\[ \Eq_\Sigma \colon \left(\Sigma_{x:A}B(x)\right) \to \left(\Sigma_{x:A}B(x)\right) \to \UU\]
by 
\[ \Eq_\Sigma(s,t) \coloneq \Sigma_{p : \pr_1(s) = \pr_1(t)} \tr_B(p, \pr_2(s)) = \pr_2(t).\]
\end{defn}

\begin{lem} The relation $\Eq_\Sigma$ is reflexive.
\end{lem}
\begin{proof} By $\Sigma$-induction it suffices to let $s : \Sigma_{x:A}B(x)$ be a pair $(a : A, b : B(a))$ in which case we have
\[ \lambda a,\lambda b. (\refl_a,\refl_b) \colon \Pi_{x :A}\Pi_{y:B(x)} \Sigma_{p : x =x} \tr_B(p,y) = y\]
since we have a definitional equality $\tr_B(\refl_a,b) \doteq b$.
\end{proof}

By path induction, we may define a map $\term{pair-eq} : s =t \to \Eq_\Sigma(s,t)$ for any $s,t : \Sigma_{x:A}B(x)$ by sending $\refl_s$ to the corresponding reflexivity term.

\begin{thm} For any type family $B \colon A \to \UU$, the map
\[ \term{pair-eq} \colon (s=t) \to \Eq_\Sigma(s,t)\]
is an equivalence for every $s,t : \Sigma_{x:A}B(x)$.
\end{thm}
\begin{proof}
We define an inverse equivalence $\term{eq-pair} \colon \Eq_\Sigma(s,t) \to (s=t)$ by repeated $\Sigma$-induction. For pairs $(a,b)$ and $(a',b')$ we must define
\[ \term{eq-pair} \colon \Sigma_{p : a =a'} \tr_B(p,b)=b' \to ((a,b) = (a',b'))\]
which we again do by $\Sigma$-induction. It suffices to  define a function in the type
\[ \Pi_{p: a = a'} (\tr_B(p,b) = b') \to ((a,b)= (a',b')).\]
By double path induction we may first assume $b'$ is $\tr_B(p,b)$ and the second path is $\refl$. Then by path induction we may assume that $a'$ is $a$ and $p$ is refl in which case we send this pair $(\refl_a,\refl_b)$ to $\refl_{(a,b)} : ((a,b) = (a,b))$.

To see that $\term{eq-pair}$ is a section of $\term{pair-eq}$ we require identifications
\[ \term{pair-eq}(\term{eq-pair}(p,q)) = (p,q)\]
for each $(p,q) : \Sigma_{p : a=a'}\tr_B(p,b)=b'$. By a double path induction it suffices to identify
\[ \term{pair-eq}(\term{eq-pair}(\refl_a,\refl_b)) = (\refl_a,\refl_b)\] and the left-hand side is judgmentally equal to the right-hand side, so we may use $\refl_{(\refl_a,\refl_b)}$.

To see that  $\term{eq-pair}$ is a retraction of $\term{pair-eq}$ we require identifications
\[ \term{eq-pair}(\term{pair-eq} (p)) = p\]
for each $p : s=t$. By path induction we may take $t$ to be $s$ and $p$ to be $\refl_s$. Then we require
\[ \term{eq-pair}(\refl_{\pr_1(s)},\refl_{\pr_2(t)}) = \refl_s.\]
By $\Sigma$-induction we may consider the case of a pair $(a,b)$ in which case we require
\[ \term{eq-pair}(\refl_a,\refl_b) = \refl_{(a,b)}.\]
Since the left-hand side we defined to be the right-hand side, we may use $\refl$.
\end{proof}

\section*{October 6: Contractibility}

Contractible types are singletons up to homotopy. In this section, we'll see several characterizations of contractibility and then relativize our discussion to include so-called ``contractible maps.''

\subsection*{Contractible types}

The term ``contractibility'' is inspired by the homotopical interpretation of type theory but it can also be thought of as an expression of ``uniqueness'': we say that a type $A$ is contractible just when it contains a unique element in a sense encoded by the Curry-Howard interpretation:

\begin{defn} A type $A$ is \textbf{contractible} if it comes equipped with a term in the type
\[ \is{contr}(A) \coloneq \Sigma_{c :A}\Pi_{x :A} c =x.\]
Given $(c,C) : \is{contr}(A)$ we refer to $c :A$ as the \textbf{center of contraction} and $C : \Pi_{x:A} c =x$ as the \textbf{contraction} or \textbf{contracting homotopy}.
\end{defn}

\begin{q} This terminology suggests that $C$ is a homotopy between some pair of functions. Which functions?
\end{q}

\begin{ex} The unit type is easily seen to be contractible. We take $\star : \1$ as the center of contraction and define $C : \Pi_{x:\1} \star = x$ by singleton induction: in the case where $x$ is $\star$, we define $C(\star) = \refl_\star$.
\end{ex}

We've met another example of a contractible type already.

\begin{thm} For any $a : A$ the type
\[ \Sigma_{x:A}a =x\]
is contractible.
\end{thm}
\begin{proof}
We take $(a,\refl_a) : \Sigma_{x:A} a =x$ to be the center of contraction. In our proof of the uniqueness of reflexivity we constructed the required contracting homotopy.
\end{proof}

\subsection*{Singleton induction}

Contractible types satisfy an induction principle similar to singleton types.

\begin{defn} Suppose $A$ comes with a term $a :A$. Then we say $A$ satisfies \textbf{singleton induction} if for every type family $B$ over $A$ the map
\[
\term{ev-pt} : \left(\Pi_{x:A} B(x)\right) \to B(a)
\]
defined by $\term{ev-pt}f \coloneq f(a)$ has a section. The data of this section is then given by a function and a homotopy
\[ \term{ind-sing}_a : B(a) \to \Pi_{x:A} B(x) \quad \text{and} \quad \term{comp-sing}_a : \term{ev-pt}\circ\term{ind-sing}_a \sim \id.\]
\end{defn}

The singleton induction principle is almost the same as the induction principle for the unit type except for one point of difference: the ``computation rule'' for singleton induction is stated using an \emph{identification} rather than a judgmental equality.  Note in particular, that $\star : \1$ satisfies singleton induction with $\lambda x . \refl_x$ as the required homotopy.

\begin{thm} For a type $A$ the following are logically equivalent.
\begin{enumerate}
\item $A$ is contractible.
\item $A$ comes equipped with a term $a : A$ satisfying singleton induction.
\end{enumerate}
\end{thm}
\begin{proof}
Suppose we have $(a,C) : \is{contr}(A) \coloneq \sum_{c :A} \prod_{x:A} c =x$. We may replace the homotopy $C : \term{const}_a \sim \id_A$ by a new homotopy $C'$ defined by $C'(x) = C(a)^{-1} \cdot C(x)$. We then have an identification $\term{left-inv} : C'(a) = \refl_a$. So without loss of generality we suppose $(a,C) : \is{contr}(A)$ comes with a term $p : C(a) = \refl_a$.

Consider a type family $B$ over $A$. For each $b : B(a)$ our goal is to define $\term{ind-sing}_a(b) : \prod_{x:A}B(x)$, which we do by
\[ \term{ind-sing}_a(b) \coloneq \tr_B(C(x),b) : B(x).\]
It remains to construct an identification $\term{ind-sing}_a(b,a) = b$. We have a function
\[ \lambda (q : a=x). \tr_B(q,b)\] that evaluates at the path $C(a)$ to give $\term{ind-sing}_a(b)$ and evalutes at the path $\refl_a$ to give $b$. When we apply this function to the path $p : C(a) = \refl_a$ we get the desired identification.

Conversely, suppose $a :A$ satisfies singleton induction. To prove that $A$ is contractible with center of contraction $a$ we apply singleton induction to the type family $B(x) \coloneq a = x$ to obtain \[\term{ind-sing}_a : a =a \to \Pi_{x:A} a=x.\] Now $\term{ind-sing}_a(\refl_a)$ is a contracting homotopy.
\end{proof}

\subsection*{Fibers}

\begin{defn} Let $f \colon A \to B$ be a function between types and consider $b : B$. The \textbf{fiber} of $f$ over $b$ is the type
\[ \fib_f(b) \coloneq \Sigma_{a:A} f(a) =b.\]
\end{defn}

That is the fiber is what in other contexts might get called the ``homotopy fiber'': it is comprised of those terms $a:A$ whose images $f(a):B$ are identified with $b$. 

It will be useful to identify the identity type of the fiber, which we do by following our usual strategy.

\begin{defn} Let $f : A \to B$ and $b : B$. Given two terms $(a,p), (a',p') : \fib_f(b)$ define a type
\[ \type{Eq-fib}_f((a,p),(a',p')) \coloneq \Sigma_{\alpha : a = a'} p = \ap_f(\alpha) \cdot p'.\]
\end{defn}

Note $\type{Eq-fib}_f :  \fib_f(b) \to \fib_f(b) \to \UU$ is reflexive since we have
\[ \lambda(a,p). (\refl_a,\refl_p) \colon \Pi_{(a,p) :\fib_f(b)} \type{Eq-fib}_f((a,p),(a,p)).\]

\begin{prop} Consider $f \colon A \to B$ and $b :B$. The canonical map
\[ (a,p) =_{\fib_f(b)} (a',p') \to \type{Eq-fib}_f((a,p),(a',p'))\]
induced by the reflexivity term is an equivalence for any pair of points $(a,p), (a',p') : \fib_f(b)$.
\end{prop}
\begin{proof}
For the inverse equivalence, we need a function
\[ \left(\Sigma_{\alpha : a = a'} p = \ap_f(\alpha) \cdot p' \right) \to (a,p) =_{\fib_f(b)} (a',p').\]
By $\Sigma$-induction it suffices to consider the image of a pair $\alpha : a = a'$ and $\beta : p = \ap_f(\alpha) \cdot p'$. By path induction we may take $p$ to be $\ap_f(\alpha) \cdot p'$ and $\beta$ to be $\refl$. By path induction again, we may take $a$ to be $a'$ and $\alpha$ to be $\refl$. Since $\ap_f(\refl)\cdot p'$ is judgmentally equal to $p'$ we may use $\refl$ as our identification between $(a',\ap_f(\refl)\cdot p')$ and $(a',p')$. The construction of the homotopies witnessing the left and right inverses are similar.
\end{proof}


\subsection*{Contractible maps and equivalences}

\begin{defn} We say that a function $f \colon A \to B$ is \textbf{contractible} if it comes equipped with a term of type
\[ \is{contr}(f) \coloneq \Pi_{b:B} \is{contr}(\fib_f(b)).\]
\end{defn}

\begin{thm} Any contractible map is an equivalence.
\end{thm}
\begin{proof}
Suppose $f \colon A \to B$ is contractible. The center of contraction $(g(b),G(B))$ in each fiber gives rise to a dependent function
\[ \lambda b. (g(b),G(B)) \colon \Pi_{b:B} \fib_f(b).\]
In particular $g : B \to A$ defines a map and $G \colon f \circ g \sim \id_B$ defines a homotopy. So $g$ is a section of $f$.

It remains to show that $g$ is also a retraction of $f$ by defining a term in the type $g \circ f \sim \id_A$. For each $a : A$ we have an identification $p : f(g(f(a))) = f(a)$ since $g$ is a section of $f$. So $(g(f(a)),p) : \fib_f(f(a))$. Since this type is contractible there is an identification $\beta : (g(f(a)),p) = (a, \refl_{f(a)})$. The base path $\ap_{\pr_1}(\beta) : g(f(a)) = a$ gives the desired identification.
\end{proof}

In fact, equivalences necessarily also define contractible maps, which we show in a few steps. Recall an \textbf{invertible map} is a map $f \colon A \to B$ equipped with $g \colon B \to A$ and homotopies $G : f \circ g \sim \id$ and $H : g \circ f \sim \id$. Then the whiskered homotopies $G \circ f$ and $f \circ H$ both have the $f \circ g \circ f \sim f$.

\begin{defn} A map $f \colon A \to B$ is \textbf{coherently invertible} if it comes with
\[ g : B \to A, \quad G : f \circ g \sim \id, \quad H : g \circ f \sim \id, \quad K : G \circ f \sim f \circ H.\]
\end{defn}

\begin{prop} Any coherently invertible map has contractible fibers.
\end{prop}
\begin{proof}
Given $f \colon A \to B$ together with
\[ g : B \to A, \quad G : f \circ g \sim \id, \quad H : g \circ f \sim \id, \quad K : G \circ f \sim f \circ H.\]
we wish to show that $\fib_f(b)$ is contractible for any $b : B$. We take $(g(b), G(b)) : \fib_f(b)$ as the center of contraction. 

For the contracting homotopy it suffices to define a term in the equivalent type
\[ \prod_{a :A} \prod_{p : f(a) = b} \type{Eq-fib}_f((g(b),G(B)), (a,p).\]
By path induction on $p$ it suffices to define a term in the type
\[ \prod_{a :A} \type{Eq-fib}_f((g(f(a)),G(f(a))),(a, \refl_{f(a)})).\]
By the definition of $\type{Eq-fib}_f$ this means that we need, for each $a:A$, a path $H(a) : g(f(a))=a$ and a further identification
\[ G(f(a)) = \ap_f(H(a)) \cdot \refl_{f(a)}.\]
We have $K(a) : G(f(a)) = \ap_f(H(a))$ so we get the identification we want by composing with $\term{right-unit-htpy}$.\end{proof}

Our next goal is to show that any invertible map $f \colon A \to B$ equipped with $g \colon B \to A$ and homotopies $G : f \circ g \sim \id$ and $H : g \circ f \sim \id$ can be improved to a coherently invertible map at the cost of replacing $G$ with a new homotopy $G'  : f \circ g \sim \id$ satisfying the coherence $K : f \circ H \sim G' \circ f$. The upshot is that we have a map
\[ \type{has-inverse}(f) \to \is{coh-invertible}(f).\]
The construction is by messy path algebra that you can read about in \cite[\S 10.4]{Rijke}.



\begin{cor} For any type $A$ and term $a :A$ the type
\[ \Sigma_{x:A} x =a\] is contractible.
\end{cor}
\begin{proof}
This type is the fiber of the identity function $\id_A \colon A \to A$ over $a :A$. Since $\id_A$ is an equivalence, this type must be contractible.
\end{proof}

Soon we'll have a second proof: we'll be able to use the equivalence $\inv \colon (a =x) \to (x =a)$ to define an equivalence $\lambda x. \inv \colon \Sigma_{x:A} a =x \to \Sigma_{x:A} x =a$. It follows easily that any type equivalent to a contractible type is contractible.

\section*{October 11: The fundamental theorem of identity types}

 \subsection*{Families of equivalences}
 
For any family of maps $f \colon \Pi_{x:A} B(x) \to C(x)$ there is a map
\[ \term{tot}(f) : \Sigma_{x:A} B(x) \to \Sigma_{x:A}C(x)\]
defined by $\lambda(x,y).(x,f(x,y))$. 

\begin{thm} For any  family of maps $f \colon \Pi_{x:A} B(x) \to C(x)$ the following are logically equivalent:
\begin{enumerate}
\item The family $f$ is a \textbf{family of equivalences}: for each $x :A$ the map $f(x)$ is an equivalence.
\item The map $\term{tot}(f)$ is an equivalence.
\end{enumerate}
\end{thm}
\begin{proof}
Recall equivalences are contractible maps: meaning maps whose fibers are all contractible. So it suffices to show that $f(x)$ is a contractible map for each $x$ if and only if $\term{tot}(f)$ is a contractible map. For this, we must show for each $x :A$ and $c : C(x)$ that $\fib_{f(x)}(c)$ is contractible if and only if $\fib_{\type{tot}(f)}(x,c)$ is contractible. But in fact these fibers are always equivalent, as the following lemma shows.
\end{proof}

\begin{lem} For any  family of maps $f \colon \Pi_{x:A} B(x) \to C(x)$ and any term $t : \sum_{x:A}C(x)$ there is an equivalence
\[
\fib_{\term{tot}(f)}(t) \simeq \fib_{f(\pr_1(t))}\pr_2(t).\]
\end{lem}
\begin{proof}
For all ${t: \Sigma_{x:A}C(x)}$ define $\phi(t) \colon  \fib_{\term{tot}(f)}(t) \to \fib_{f(\pr_1(t))}\pr_2(t)$ by pattern matching by taking $(x,f(x,y),(x,y), \refl)$ to $(y,\refl)$. To see that $\phi(t)$ is an equivalence for each $t : \Sigma_{x:A}C(x)$ we construct an inverse by pattern matching \[ \phi(x,f(x,y),y, \refl) \coloneq ((x,y),\refl)\] and homotopies by pattern matching in which case both homotopies reduce to $\refl$.
\end{proof}

Now consider a closely related situation where we are given a map $f \colon A \to B$ and a family $C \colon B \to \UU$. We have a map
\[ \lambda(x,z).(f(x),z): \Sigma_{x:A} C(f(x)) \to \Sigma_{y :B}C(y).\]
Again, by the same style of argument, if $f$ is an equivalence then this map is an equivalence (because the fibers are equivalent), but in this case the converse does not hold: consider $\true : \1 \to \bool$ and the type family $\false = b : \bool \to \UU$.

Nevertheless we can use the one-sided implication to extend the previous theorem as follows. Given $f \colon A \to B$ and a family of maps $g : \Pi_{x:A}C(x) \to D(f(x))$ where $C$ is a type family over $A$ and $D$ is a type family over $B$, we say that $g$ is a \textbf{family of maps over} $f$. Define
\[ \term{tot}_f(g) : \Sigma_{x:A}C(x) \to \Sigma_{y:B}D(y)\] by $\term{tot}_f(g)(x,z) \coloneq (f(x),g(x,z))$.

\begin{thm} Suppose $g$ is a family of maps over $f$ and $f$ is an equivalence. Then the following are logically equivalent:
\begin{enumerate}
\item The family of maps $g$ over $f$ is a family of equivalences.
\item The map $\term{tot}_f(g)$ is an equivalence.
\end{enumerate}
\end{thm}
\begin{proof}
We have a commuting triangle of maps
\[
\begin{tikzcd} \Sigma_{x:A}C(x) \arrow[rr, "\term{tot}_f(g)"] \arrow[dr, "\term{tot}(g)"'] & & \Sigma_{y:B}D(y) \\ & \Sigma_{x:A}D(f(x)) \arrow[ur, "{\lambda(x,z).(f(x),z)}"'] 
\end{tikzcd}
\]
Since $f$ is an equivalence, the bottom right map is an equivalence. The equivalences are closed under the 2-of-3 property (meaning if any two of a composable pair and their composite are equivalences so is the third map). Thus $\term{tot}(g)$ is an equivalence if and only if $\term{tot}_f(g)$ is an equivalence. And by the previous theorem, the first condition asserts that $g$ is a family of equivalences.
\end{proof}

\subsection*{The fundamental theorem}

The fundamental theorem of identity types provides necessary and sufficient conditions for a type family $B \colon A \to \UU$ and terms $a : A$ and $b : B(a)$ to define a family of equivalences $\Pi_{x:A}(a=x) \simeq B(x)$ by $(a,\refl)\mapsto b$. In fact, we'll see we can be a bit less particular about how exactly the family of equivalences is defined.

\begin{defn} Given a type $A$ and a term $a:A$ a \textbf{(unary) identity system} on $A$ at $a$ is given by a type family $B : A \to \UU$ and a term $b : B(a)$ so that for any family of types $P \colon \Sigma_{x:A}B(x) \to \UU$ the function
\[ \ev_{a,b} \colon \Pi_{x:A} \Pi_{y:B(x)}P(x,y) \to P(a,b)\] has a section.
\end{defn}

That is, if $(B,b)$ is an identity system at $(A,a)$ and $P$ is a family of types over $x:A$ and $y:B(x)$ then for each $p : P(a,b)$ there is a dependent function $f :\Pi_{x:A} \Pi_{y:B(x)} P(x,y)$ so that $f(a,b) = p$. This is a variant of the path induction principal where the computation rule is given by an identification.

\begin{thm}[fundamental theorem of identity types] Let $A$ be a type, let $a :A$, and let $B : A \to \UU$. Then the following are logically equivalent for any family of maps \[ f : \Pi_{x:A}(a=x) \to B(x).\]
\begin{enumerate}
\item $f$ is a family of equivalences.
\item The total space $\Sigma_{x:A}B(x)$ is contractible.
\item The family $B$ is an identity system.
\end{enumerate}
In particular, for any $b : B(a)$ the canonical map \[\term{path-ind}_a(b) \colon \Pi_{x:A}(a=x)\to B(x)\] is a family of equivalences if and only if $\Sigma_{x:A}B(x)$ is contractible. 
\end{thm}
\begin{proof}
By our theorem characterizing families of equivalences $f$ is a family of equivalences iff $\term{tot}(f)$ induces an equivalence
\[ \left(\Sigma_{x:A}(a=x) \right) \simeq \left(\Sigma_{x:A}B(x)\right).\]
The left-hand type is contractible so this is the case if and only if $\Sigma_{x:A}B(x)$ is contractible. This proves the equivalence of (i) and (ii). 

For the equivalence of (ii) and (iii) consider the commutative triangle:
\[
\begin{tikzcd} \Pi_{t : \Sigma_{x:A}B(x)} P(t) \arrow[rr, "\term{ev-pair}"] \arrow[dr, "\term{ev-pt}_{(a,b)}"'] & & \Pi_{x:A} \Pi_{y : B(x)} P(x,y) \arrow[dl, "\ev_{(a,b)}"] \\ & P(a,b)
\end{tikzcd}
\]
By $\Sigma$-induction, the top map has a section. It follows that the left map has a section if and only if the right map has a section. The left-hand section is the universal property called singleton induction that is satisfied if and only if the type $\Sigma_{x:A}B(x)$ is contractible, while the right-hand section is the universal property of an identity system. This proves the equivalence of  (ii) and (iii).
\end{proof}

\subsection*{Equality on \texorpdfstring{$\bN$}{N}}

As our first application recall the observational equality type family $\Eq_\bN : \bN \to \bN \to \UU$ satisfying
\[ \Eq_\bN(0_\bN,0_\bN) \doteq \1 \quad \Eq_\bN(\suc(n),0_\bN) \doteq \emptyset \quad \Eq_\bN(0,\suc(n)) \doteq \emptyset \quad \Eq_\bN(\suc(m), \suc(n)) \doteq \Eq_\bN(m,n).\] We previously showed that this type family is logically equivalent to the identity type family for the natural numbers, but we can do better. Using the reflexivity term $\term{refl-Eq}_\bN : \prod_{n :\bN} \Eq_{\bN}(n,n)$ we have a canonical map $(m=n) \to \Eq_\bN(m,n)$ defined by path induction.

\begin{thm} For all $m,n : \bN$ the canonical map
\[ (m=n) \to \Eq_\bN(m,n)\] is an equivalence.
\end{thm}
\begin{proof}
It suffices to show for each $m : \bN$ that the type \[ \Sigma_{n : \bN} \Eq_\bN(m,n)\] is contractible. We take $(m,\term{refl-Eq}_\bN(m))$ as the center of contraction.

The contracting homotopy 
\[ \gamma(m) : \Pi_{n :\bN} \Pi_{e :\Eq_\bN(m,n)} (m,\term{refl-Eq}_\bN(m)) = (n,e)\]
is defined by induction on $m,n : \bN$ from the base case $\gamma(0,0, \star) \coloneq \refl$. If either $m$ or $n$ is 0 and the other is a successor we can define this using ex-falso. 

In the inductive step we seek an identification $\gamma(m+1,n+1,e) : (m+1, \term{refl-Eq}_\bN(m+1)) = (n+1,e)$. To define this we use the map
\[ \lambda(n,e).(n+1,e) : \Sigma_{n:\bN}\Eq_\bN(m,n) \to \Sigma_{n : \bN}\Eq_\bN(m+1,n).\]
Since this map carries $(m,\term{refl-Eq}_\bN(m))$ to $(m+1,\term{refl-Eq}_\bN(m+1))$ we can apply the map to the identification $(m,n,e)$ to get the identification we seek.
\end{proof}

\subsection*{Embeddings}

Our next application will show that equivalences are embeddings, defined as follows:

\begin{defn} An \textbf{embedding} is a map $f \colon A \to B$ that satisfies the property that 
\[ \ap_f : (x=y) \to (f(x)=f(y))\]
is an equivalence for every $x,y :A$.
\end{defn}

Write 
\[ \is{emb}(f) \coloneq \Pi_{x,y:A} \is{equiv}(\ap_f : (x=y) \to (f(x)=f(y))).\]

\begin{thm} Equivalences are embeddings.
\end{thm}
\begin{proof}
Suppose $e : A \simeq B$ is an equivalence and $x:A$. We wish to show that 
\[ \ap_e : (x=y) \to (e(x)=e(y))\] is an equivalence for every $y :A$. For this it suffices to show that $\Sigma_{y:A}e(x) =e(y)$ is contractible. We have an equivalence
\[ \Sigma_{y:A}e(x)=e(y) \simeq \Sigma_{y:A} e(y) = e(x)\]
and the latter type is the fiber $\fib_e(e(x))$. Since $e$ is an equivalence this fiber is contractible so the result follows.
\end{proof}

\subsection*{Disjointness of coproducts}

For a third application, we characterize the identity types of coproducts.

\begin{thm} Let $A$ and $B$ be types. Then for any $a,a' :A$ and $b,b': B$ there are equivalences
\begin{align*} (\inl(a) = \inl(a')) &\simeq (a = a')  \\ (\inl(a) = \inr(b)) &\simeq \emptyset \\ (\inr(b) = \inl(a)) &\simeq \emptyset \\ (\inr(b) = \inr(b')) &\simeq (b = b')
\end{align*}
\end{thm}

We follow our usual strategy first defining a type family
\[ \type{Eq-+}_{A,B} : (A+B) \to (A+B) \to \UU\] 
by induction by
\begin{align*} \type{Eq-+}_{A,B}(\inl(a),\inl(a')) &\simeq (a = a')  \\ \type{Eq-+}_{A,B}(\inl(a),\inr(b)) &\simeq \emptyset \\ \type{Eq-+}_{A,B}(\inr(b),\inr(a)) &\simeq \emptyset \\ \type{Eq-+}_{A,B} (\inr(b), \inr(b')) &\simeq (b = b')
\end{align*}

Again by induction there is a term $\term{Eq-+-refl} : \Pi_{z:A+B} \type{Eq-+}_{A,B}(z,z)$ defined by $\refl$ in both cases. Thus there is a map
\[ \term{Eq-+-eq}: \Pi_{s,t:A+B} (s=t) \to \type{Eq-+}_{A,B}(s,t)\]
defined by path induction. 

\begin{prop} For any $s : A +B$ the total space
\[ \Sigma_{t:A+B}\type{Eq-+}_{A,B}(s,t)\]
is contractible.
\end{prop}
\begin{proof}
By induction on $s$ we have to consider two cases, of which we prove just one: that 
\[ \Sigma_{t:A+B}\type{Eq-+}_{A,B}(\inl(a),t)\]
is contractible. From distributivity of dependent pairs of coproducts we have
\[ \Sigma_{t:A+B}\type{Eq-+}_{A,B}(\inl(a),t) \simeq \Sigma_{x:A} \type{Eq-+}_{A,B}(\inl(a),\inl(x)) + \Sigma_{b:B} \type{Eq-+}_{A,B}(\inl(a),\inr(b)) \simeq \Sigma_{x:A} (a = \inl(x)) + \Sigma_{b:B} \emptyset\]
\[ \simeq  \Sigma_{x:A} (a = \inl(x)) + \emptyset \simeq  \Sigma_{x:A} (a = \inl(x)).  \]
This last type is contractible so the first type is as claimed.
\end{proof}

This establishes the desired family of equivalences of types.

\subsection*{The structure identity principle}

The notion of identity system $B : A \to \UU$ and $b : B(a)$ over a type $A$ and term $a:A$ can be extended to a notion of dependent identity system. A \textbf{dependent identity system} over $(B,b)$ is given by a type family $C : A \to \UU$ together with 
\[ D : \Pi_{x:A} B(x) \to C(x) \to \UU\] and a term $c : C(a)$ so that $z \mapsto D(a,b,z)$ is an identity system at $c$. We leave the details to \cite[\S11.6]{Rijke}.

\section*{October 13: Propositions and sets}

We now formally study propositions in homotopy type theory.

\subsection*{Propositions}

\begin{defn} A type $A$ is a \textbf{proposition} if all of its identity types are contractible: i.e., if it comes with a term of type
\[ \is{prop}(A) \coloneq \Pi_{x,y:A} \is{contr}(x=y).\]
\end{defn}

Given a universe $\UU$ we define $\Prop$ to be the type of all small propositions:
\[ \Prop \coloneq \Sigma_{X: \UU}\isprop(X).\]

\begin{ex} We have shown that identity types of contractible types are contractible. Thus contractible types are propositions.
\end{ex}

\begin{ex} The empty type is also a proposition, for ex-falso inhabits
\[ \Pi_{x,y: \emptyset} \iscontr(x=y).\]
\end{ex}

There are many equivalent ways to assert that a type is a proposition.

\begin{prop} For a type $A$, the following are logically equivalent:
\begin{enumerate}
\item $A$ is a proposition.
\item Any two terms of type $A$ can be identified: i.e., there is a dependent function in the type
\[ \Pi_{x,y:A} x=y.\]
\item The type $A$ is contractible as soon as it is inhabited: i.e., there is a term of type $A \to \iscontr(A)$.
\item The map $\term{const}_\star : A \to \1$ is an embedding. 
\end{enumerate}
\end{prop}
\begin{proof}
(i) clearly implies (ii), by using the center of contraction. Assuming (ii) we have $p : \Pi_{x,y:A} x= y$. Note for any $x$ that $p(x) : \Pi_{y :A} x=y$ is a contracting homotopy onto $x$. Thus, we have a function
\[ \lambda x. (x,p(x)) : A \to \iscontr(A).\]

Next assume (iii) and consider a function $c : A\to \iscontr(A)$. To prove 
\[ \Pi_{x,y :A} \isequiv (\ap_{\term{const}_\star} : (x=y) \to (\star = \star))\] it suffices to prove
\[ A \to \left(\Pi_{x,y :A} \isequiv (\ap_{\term{const}_\star} : (x=y) \to (\star = \star))\right)\]
because then we can use this function $f$ applied to one of the two terms $x,y : A$ to get the data we want. But now our new goal allows us to assume we have a term $a:A$ and so it follows from our assumption that $A$ is contractible. Thus $\term{const}_\star : A \to \1$ is an equivalence and in particular an embedding. This proves (iv).

Finally, $\term{const}_\star : A \to \1$ is an embedding, then the types $(x=y)$ and $(\star=\star)$ are equivalent. The latter type is contractible so the former must be as well. This proves that (iv) implies (i).
\end{proof}

A useful feature of propositions is that logical equivalences become equivalences:

\begin{prop} For propositions $P$ and $Q$
\[ (P \simeq Q) \iff (P \iff Q).\]
\end{prop}
\begin{proof}
Clearly we have $(P \simeq Q) \to (P \iff Q)$ so the content is in the converse. Given $f : P \to Q$ and $g : Q \to P$ we obtain homotopies $f \circ g \sim \id$ and $g \circ f \sim \id$ by the fact that any two elements in $P$ and $Q$ can be identified. Thus, $f$ and $g$ are equivalence inverses.
\end{proof}

\subsection*{Subtypes}

Now that we know about propositions we can say that a type family $P \colon A \to \UU$ is a predicate if for each $a :A$, $P(a)$ is a proposition. In other words, predicates are type families $P \colon A \to \Prop$. Other terminology is commonly in use in this situation.

\begin{defn} A type family $B$ over $A$ is a \textbf{subtype} of $A$ if for each $x:A$, $B(x)$ is a proposition. In this situation we say that $B(x)$ is a \textbf{property} of $x:A$.
\end{defn}

We'll show that for subtypes $B$ over $A$ the map $\pr_1 \colon \Sigma_{x:A} B(x) \to A$ is an embedding. It follows, then, that $(x,y) = (x',y')$ if and only if $x=_Ax'$. To prove this first observe:

\begin{lem} Suppose $e : A\simeq B$. Then 
\[ \isprop{A} \iff \isprop{B}.\]
\end{lem}

\begin{proof}
Since $e$ is an equivalence, $e$ is an embedding, meaning that $\ap_e : (x=_Ay) \to (e(x)=_B e(y))$ is an equivalence. Now if $B$ is contractible then $(e(x)=_Be(y))$ is contractible which then implies that $(x=_Ay)$ is contractible. Thus $\isprop(B) \to \isprop(A)$. The converse is proven similarly using the inverse equivalence to $e$.
\end{proof}

\begin{thm} For $f : A \to B$ the following are logically equivalent:
\begin{enumerate}
\item $f$ is an embedding.
\item $\fib_f(b)$ is a proposition for all $b : B$
\end{enumerate}
\end{thm}
\begin{proof}
By the fundamental theorem of identity types, $f$ is an embedding if and only if $\Sigma_{x:A}f(x)=_B f(y)$ is contractible for each $y :A$. Thus $f$ is an embedding iff $\fib_f(f(y))$ is contractible for each $y : A$. If $b:B$ and $p : f(y) = b$ then transport defines an equivalence
\[ \fib_f(f(y)) \simeq \fib_f(b).\]
Thus $f$ is an embedding iff $\fib_f(b)$ is contractible for each $b : B$ equipped with $p : f(y) = b$ for some $y:A$. This latter condition may be re-expressed as \[ \fib_f(b) \to \iscontr(\fib_f(b)),\]
which is logically equivalent to the assertion that each $\fib_f(b)$ is a proposition.
\end{proof}

\begin{cor} For any family $B : A \to \UU$ the following are logically equivalent:
\begin{enumerate}
\item $\pr_1 \colon \Sigma_{x:A} B(x) \to A$ is an embedding.
\item $B(x)$ is a proposition for each $x:A$.
\end{enumerate}
\end{cor}
\begin{proof}
Since $\fib_{\pr_1}(x) \simeq B(x)$ this follows immediately from the previous theorem.
\end{proof}

\subsection*{Sets}

\begin{defn} A type $A$ is a \textbf{set} if its identity types are propositions:
\[ \is{set}(A) \coloneq \Pi_{x,y:A} \isprop(x=y).\]
\end{defn}

\begin{ex} The type of natural numbers is a set, since we have $(m=n) \simeq \Eq_\bN(m,n)$ and, by induction, the latter types are propositions.
\end{ex}

\begin{thm} For a type $A$ the following are logically equivalent:
\begin{enumerate}
\item $A$ is a set.
\item $A$ satisfies \textbf{axiom K}: that is, $A$ comes with a term in the type
\[ \type{axiom-K}(A) \coloneq \Pi_{x:A} \Pi_{p : x =x} \refl_x = p.\]
\end{enumerate}
\end{thm}
\begin{proof}
If $A$ is a set then $x=x$ is a proposition so any two terms in it can be identified.

Conversely, if axiom-K holds then for any $p,q : x =y$ we can identified $p \cdot q^{-1}$ and $\refl_x$ and it follows that $p=q$. This proves that $x=y$ is a proposition so $A$ must be a set.
\end{proof}

The following result can be used to prove that a type $A$ is a set.

\begin{thm} Let $A$ be a type and suppose $R \colon A \to A \to \UU$ satisifes:
\begin{enumerate}
\item Each $R(x,y)$ is a proposition.
\item $R$ is reflexive, witnessed by $\rho : \Pi_{x:A} R(x,x)$.
\item There is a map $R(x,y) \to (x=y)$ for all $x,y:A$.
\end{enumerate}
Then any family of maps $\Pi_{x,y:A} (x=y) \to R(x,y)$ is a family of equivalences and $A$ must be a set.
\end{thm}
\begin{proof}
By hypothesis we have terms $f : \Pi_{x,y:A} R(x,y) \to (x=y)$ and also $\term{path-ind}(\rho) : \Pi_{x,y} (x=y) \to R(x,y)$. Since each $R(x,y)$ is a proposition we have a homotopy $\term{path-ind}(\rho)(x,y) \circ f(x,y) \sim \id_{R(x,y)}$ proving that $R(x,y)$ is a retract of $x=y$. Thus, $\Sigma_{y:A} R(x,y)$ is a retract of $\Sigma_{y:A} x=y$. Since the latter type is contractible the former must be too. Thus any family of maps $\Pi_{y:A}(x=y) \to R(x,y)$ is a family of equivalences (since its totalization is a map between contractible types and thus an equivalence). 

But now we know that the identity types of $A$ are propositions so $A$ must be a set.
\end{proof}

Recall a type $A$ has decidable equality  if the identity type $x=_Ay$ is decidable for every $x, y : A$, meaning
\[ \Pi_{x,y:A} (x=y) + \neg (x=y).\]

\begin{thm}[Hedberg] Any type with decidable equality is a set.
\end{thm}
\begin{proof} Let $d : \Pi_{x,y:A} (x=y) + \neg (x=y)$ be a witness to the fact that $A$ has decidable equality and let $\UU$ be a universe containing $A$.

Define a type family $R'(x,y) : ((x=y) +\neg(x=y)) \to \UU$ by
\[ R'(x,y, \inl(p)) \coloneq \1 \quad R'(x,y, \inr(p)) \coloneq \emptyset.\]
Note that this is a family of propositions. Now define $R(x,y) \coloneq R'(x,y,d(x,y))$. This defines a family of propositions $R : A \to A \to \UU$. This is a reflexive binary relation so the apply the previous theorem to conclude that $A$ is a set we must only show that $R$ implies identity. 

Since $R$ is defined to be an instance of $R'$ it suffices to construct a function for each $q : (x=y) + \neg(x=y)$ that proves $f(q) : R'(x,y,q) \to (x=y)$. We have this by
\[ f(\inl(p,r)) \coloneq p \qquad f(\inr(p),r) \coloneq \term{ex-falso}(r). \qedhere\]
\end{proof}

\section*{October 18: General Truncation Levels \& Function extensionality}

\subsection*{General truncation levels}

So far we have defined:
\begin{align*}
\iscontr(A) &\coloneq \Sigma_{a:A} \Pi_{x:A} a=x\\
\isprop(A) &\coloneq \Pi_{x,y:A} \iscontr(x=y) \\
\is{set}(A) &\coloneq \Pi_{x,y:A} \isprop(x=y) 
\end{align*}
These define the first few layers of the hierarchy of truncation levels. This hierarchy starts at level -2 with the contractible types and continues at level -1 with the propositions. This makes level 0 the sets, which are typically thought of as ``0-dimensional.''

Let $\bT$ be the inductive type with constructors $-2_\bT : \bT$ and $\term{succ}_\bT : \bT \to \bT$. The natural inclusion $i \colon \bN \to \bT$ is defined recursively by $i(0_\bN) \coloneq \term{succ}_\bT (\term{succ}_\bT (-2_\bT))$ and $i(\suc(n)) \coloneq \term{succ}_\bT (i(n))$. We abbreviate by writing $-2,-1,0,1,2,\ldots$ for the first few terms of $\bT$ when the context is clear.

\begin{defn} Define $\is{trunc} : \bT \to \UU \to \UU$ recursively by
\[ \is{trunc}_{-2}(A) \coloneq \iscontr(A) \qquad \is{trunc}_{k+1}(A) \coloneq \Pi_{x,y:A} \is{trunc}_k(x=_Ay).\]
When $\is{trunc}_k(A)$ holds we say $A$ is \textbf{$k$-truncated} or is a \textbf{$k$-type}. You can prove, inductively, in $k : \bT$, that this is logically equivalent of the universe being used to define $\is{trunc}_k(A)$.
\end{defn}



For $k \geq 0$, we may also say that $A$ is a \textbf{proper $k$-type} if $\is{trunk}_k(A)$ holds but $\is{trunk}_{k-1}(A)$ does not.


\begin{defn} A map $f \colon A \to B$ is \textbf{$k$-truncated} if its fibers are $k$-truncated.
\end{defn}

Given a universe $\UU$, we may also define a universe of $k$-truncated types by
\[ \UU^k \coloneq \Sigma_{X:\UU} \is{trunc}_k(X).\]


The truncation levels are successively contained in one another:

\begin{prop} If $A$ is a $k$-type then $A$ is also a $k+1$-type.
\end{prop}
\begin{proof}
We use induction on $k : \bT$. In the base case, we have shown already that contractible types are propositions. For the inductive step, note that if any $k$-type is a $k+1$-type then this applies to show that the identity types of a $k+1$-type, which are known to be $k$-types, are also $k+1$-types. This proves that any $k+1$-type is a $k+2$-type.
\end{proof}

In particular:

\begin{cor} If $A$ is a $k$-type its identity types are also $k$-types.
\end{cor}

General truncation levels are stable under equivalence:

\begin{prop} If $e : A \simeq B$ and $B$ is a $k$-type so is $A$.
\end{prop}
\begin{proof}
We know this for contractible types, which is the base case. For the inductive step, $e: A \simeq B$ provides an equivalence $\ap_e :(x=y) \to (e(x)=e(y))$ for any $x,y:A$. If $B$ is a $k+1$-type its identity types are $k$-types so the inductive hypothesis implies that $(x=y)$ is also a $k$-type. This proves that $A$ is a $k+1$-type.
\end{proof}

A similar argument shows:

\begin{cor} If $f \colon A \to B$ is an embedding and $B$ is a $k+1$-type, then so is $A$.
\end{cor}

Our final theorem generalizes the result that shows that a map is an embedding if and only if its fibers are propositions.
\begin{thm} For $f \colon A \to B$ the following are logically equivalent:
\begin{enumerate}
\item $f$ is $(k+1)$-truncated.
\item For each $x,y:A$, $\ap_f :(x=y) \to (f(x)=f(y))$ is $k$-truncated.
\end{enumerate}
\end{thm}
\begin{proof}
Both directions use the characterization of identity types of fibers:
\[ ((x,p) =_{\fib_f(b)}(y,q) ) \simeq \Sigma_{\alpha: x=y} p = \ap_f(\alpha) \cdot q.\]

The first statement is about identity types of fibers so consider $s,t : \fib_f(b)$.  We claim there is an equivalence \[ (s=t) \simeq \fib_{\ap_f}(\pr_2(s)\cdot \pr_2(t)^{-1}).\] By $\Sigma$-induction we can construct this for pairs $(x,p), (y,q) : \fib_f(b)$ for which we calculate
\begin{align*} ((x,p)=(y,q)) &\simeq \Sigma_{\alpha : x= y} p = \ap_f(\alpha)\cdot q  \\ &\simeq \Sigma_{\alpha:x=y} \ap_f(\alpha)\cdot q = p \\ &\simeq \Sigma_{\alpha :x=y} \ap_f(\alpha) = p \cdot q^{-1} \\
&\eqcolon \fib_{\ap_f}(p\cdot q{^{-1}}).
\end{align*}
It follows that if $\ap_f$ is $k$-truncated then so each identity type $(s=t)$ is equivalent to a $k$-truncated type and thus $f$ is $k+1$-truncated.

For the converse, we have an equivalence between $\fib_{\ap_f}(p)$ and the identity type $(x,p) =(y,\refl_{f(y)})$ between terms in $\fib_f(f(y))$. So if $f$ is $(k+1)$-truncated these fibers are $k+1$-truncated and thus  their identity types are $k$-types. This proves that the fiber $\fib_{\ap_f}(p)$ is $k$-truncated.
\end{proof}

\subsection*{Function extensionality}

The function extensionality principle characterizes the identity type of an arbitrary dependent function type, asserting that the type $f=g$ of identifications between dependent functions $f,g : \Pi_{x:A} B(x)$ is equivalent to the type of homotopies $f \sim g$. It has several equivalent forms:

\begin{prop} For a type family $B : A \to \UU$ the following are logically equivalent:
\begin{enumerate}
\item The \textbf{function extensionality principle holds} for $f,g : \Pi_{x:A}B(x)$: the family of maps
\[ \term{htpy-eq} : (f=g) \to (f \sim g)\]
defined by sending $\refl$ to $\term{refl-htpy}$ is a family of equivalences.
\item For any $f : \Pi_{x:A}B(x)$, the total space
\[ \Sigma_{g : \Pi_{x:A}B(x)} f \sim g\]
is contractible.
\item The principle of \textbf{homotopy induction} holds: for any family of types $P$ depending on $f,g : \Pi_{x:A}B(x)$ and $H : f \sim g$ the evaluation function
\[ \ev\colon \left( \Pi_{f,g : \Pi_{x:A}B(x)} \Pi_{H : f \sim g} P(f,g,H) \right) \to \Pi_{f : \Pi_{x:A}B(x)} P(f,f, \term{refl-htpy}_f)\]
has a section.
\end{enumerate}
\end{prop}
\begin{proof}
This follows by applying the fundamental theorem of identity types to the type $\Pi_{x:A}B(x)$, term $f : \Pi_{x:A}B(x)$, and type family $g : \Pi_{x:A}B(x) \vdash f \sim g \univ$.
\end{proof}

A fourth equivalent condition is more surprising because it appears to express only a weak function extensionality principle.

\begin{thm} For any universe $\UU$ the following are logically equivalent:
\begin{enumerate}
\item The \textbf{function extensionality principle} holds in $\UU$: for any type family $B$ over $A$ and dependent functions the map 
\[ \term{htpy-eq} : (f=g) \to (f \sim g)\]
is an equivalence.
\item The \textbf{weak function extensionality principle} holds in $\UU$:  for any type family $B$ over $A$ one has
\[ \left( \Pi_{x:A} \is{contr}(B(x)) \right) \to \is{contr}\left( \Pi_{x:A}B(x)\right).\]
\end{enumerate}
\end{thm}
\begin{proof}
Assume (i) and suppose each fiber $B(x)$ is contractible with center of contraction $c(x)$ and contracting homotopy $C_x : \Pi_{y : B(x)} c_x = y$. Define $c = \lambda x. c(x)$ to be the center of contraction of $\Pi_{x:A}B(x)$. For the contraction we require a term of type
\[ \Pi_{f : \Pi_{x : A}B(x)} c =f.\] By function extensionality we have a map $(c \sim f) \to (c=f)$ so it suffices to construct a term of type $c \sim f \coloneq \Pi_{x:A} c(x)= f(x)$ and $\lambda x. C_x(f(x))$ is just such a term.

For the converse, assume (ii). By the previous result it suffices to show that the type
\[ \Sigma_{g : \Pi_{x:A}B(x)} f \sim g\]
is contractible. Note we have a section-retraction pair:
\[ \left(\Sigma_{g : \Pi_{x:A}B(x)} f \sim g \right) \xrightarrow{s} \left( \Pi_{x:A}\Sigma_{y:B(x)} f(x) = y\right) \xrightarrow{r} 
\left(\Sigma_{g : \Pi_{x:A}B(x)} f \sim g \right) \]
defined by
\[ s \coloneq \lambda (g,H). \lambda x.(g(x),H(x)) \quad \text{and} \quad r \coloneq \lambda p. (\lambda x. \pr_1(p(x)), \lambda x. \pr_2(p(x))).\]
The composite is homotopic to the identity function by the computation rules for $\Sigma$ and $\Pi$-types. Here the central type is a product of contractible types so must be contractible by (ii). Since retracts of contractible types are contractible, the claim follows.
\end{proof}

Henceforth, we will assume the function extensionality principle as an axiom:

\begin{ax}[function extensionality] For any type family $B$ over $A$ and any pair of dependent functions $f,g : \Pi_{x:A}B(x)$ the map
\[ \term{htpy-eq} : (f=g) \to (f \sim g)\]
is an equivalence, with inverse $\term{eq-htpy}$.
\end{ax}

That is, we add the following rule to type theory:
\[
\inferrule{ \Gamma, x:A \vdash B(x)\univ \\ \Gamma \vdash f : \Pi_{x:A}B(x) \\ \Gamma \vdash g : \Pi_{x:A}B(x)}
{ \Gamma \vdash \term{funext} : \is{equiv}(\term{htpy-eq}_{f,g})}
\]

There are myriad consequences of the function extensionality axiom. Firstly:

\begin{thm} For any type family $B$ over $A$ one has
\[ \left( \Pi_{x:A} \is{trunc}_k(B(x)) \right) \to \is{trunc}_k\left( \Pi_{x:A}B(x)\right).\]
\end{thm}
\begin{proof}
The theorem states that $k$-types are closed under arbitrary dependent products. We prove this by induction on $k \geq -2$. The base case is the weak function extensionality principle.

For the inductive step assume $k$-types are closed under products and consider a family $B$ of $(k+1)$-types. To show that $\Pi_{x:A}B(x)$ is $(k+1)$-truncated we must show that $f=g$ is $k$-truncated for every $f,g : \Pi_{x:A}$. By function extensionality this is equivalent to the type $f \sim g \coloneq \Pi_{x:A} f(x)=g(x)$ which is defined to be a dependent product of $k$-truncated types and thus is $k$-truncated by hypothesis. Since $k$-truncated types are closed under equivalences, the result follows.
\end{proof}

For a non-dependent family we conclude that: 
\begin{cor} Suppose $B$ is a $k$-type. Then the type of functions $A \to B$ is a $k$-type for any type $A$.
\end{cor}

In particular, $\neg A$ is a proposition for any type $A$!

\section*{October 20: Universal properties}

We can understand the function extensionality axiom as providing a characterization of the identity type of a $\Pi$-type: up to equivalence, for $f,g : \Pi_{x:A}B(x)$, the identity type $f=g$ is equivalent to the type of homotopies $f \sim g$.

\subsection*{The type theoretic axiom of choice}

 First, observe that $\Pi$-types distribute over $\Sigma$-types by the type theoretic axiom of choice.

\begin{thm} For any family of types $x :A, y : B(x) \vdash C(x,y) \univ$ the map
\[ \term{choice} : \left(\Pi_{x:A}\Sigma_{y:B(x)}C(x,y) \right) \to \left( \Sigma_{f: \Pi_{x:A}B(x)} \Pi_{x:A} C(x,f(x))\right)\]
defined by 
\[ \term{choice}(h) \coloneq (\lambda x. \pr_1(h(x)), \lambda x.\pr_2(h(x)))\]
is an equivalence.
\end{thm}

Consequently, whenever we have types $A$ and $B$ and a type family $C$ over $B$ there is an equivalence
\[ \left( A \to \Sigma_{y:B}C(y) \right) \simeq \left( \Sigma_{f:A \to B} \Pi_{x:A} C(f(x))\right)\]

\begin{proof}
Define the inverse map $\term{choice}^{-1}$ by
\[ \term{choice}^{-1}(f,g) \coloneq \lambda x.(f(x).g(x)).\]
For the first homotopy it suffices to define an identification $\term{choice}(\term{choice}^{-1}(f,g)) = (f,g)$. The left-hand side computes to
\[ \term{choice}(\term{choice}^{-1}(f,g))  \doteq \term{choice}(\lambda x.(f(x).g(x))) \doteq (\lambda x.f(x),\lambda x.g(x))\]
which is definitely equal to the right-hand side by the computation rules for function types.

For the second homotopy, we require an identification $\term{choice}^{-1}(\term{choice}(h)) = h$. The left-hand side computes to
\[ \term{choice}^{-1} (\lambda x. \pr_1(h(x)), \lambda x.\pr_2(h(x))) \doteq \lambda x.(\pr_1(h(x)), \pr_2(h(x))).\]
We do not have a definitional equality relating $h(x)$ and $(\pr_1(h(x)), \pr_2(h(x)))$ but in our characterization of the identity type of $\Sigma$-types we do have an identification between them called $\term{eq-pair}(\refl,\refl)$. By function extensionality, the homotopy $\lambda x. \term{eq-pair}(\refl,\refl) : \term{choice}^{-1}(\term{choice}\sim h$ can be turned into an identification and thus a homotopy $\term{choice}^{-1} \circ \term{choice} \sim \id$.
\end{proof}

\subsection*{Universal properties}

More generally, the function extensionality axiom allows us to prove universal properties, which characterize maps out of or into a given type, and characterize that type up to equivalence. Some examples follow.

In our first example, we consider the maps out of $\Sigma$-types. The universal property states that the map
\[ \term{ev-pair} : \left( \left( \Sigma_{x:A}B(x)\right) \to C \right) \to \left( \Pi_{x:A}B(x) \to C \right)\]
given by $f \mapsto \lambda x.\lambda y. f(x,y)$ is an equivalence for any type $C$. But the analogous result is true for type familyes $C$ that depend on the type $\Sigma_{x:A}B(x)$ so we prove the result in that form.

\begin{thm}[universal property of \texorpdfstring{$\Sigma$}{Sigma}-types]
Let $B$ be a type family over $A$ and let $C$ be a type family over $\Sigma_{x:A}B(x)$. Then the map 
\[ \term{ev-pair} : \left( \Sigma_{z: \Sigma_{x:A}B(x)} C(z) \right) \to \left( \Pi_{x:A}\Pi_{y:B(x)} C(x,y) \right)\]
given by $f \mapsto \lambda x.\lambda y. f(x,y)$ is an equivalence
\end{thm}
\begin{proof}
The inverse map is given by the induction principle for $\Sigma$-types:
\[ \ind_\Sigma : \left( \Pi_{x:A}\Pi_{y:B(x)} C(x,y) \right) \to \left( \Sigma_{z: \Sigma_{x:A}B(x)} C(z) \right) .\]
By the computation rule for $\Sigma$ types, we have the homotopy 
\[ \term{refl-htpy} : \term{ev-pair} \circ\ind_\Sigma \sim \id,\]
which shows that $\ind_\Sigma$ is a section of $\term{ev-pair}$.

Function extensionality is used to construct the other homotopy. To define a homotopy $\ind_\Sigma \circ \term{ev-pair} \sim \id$ requires identifications $\ind_\Sigma(\lambda x. \lambda y. f(x,y)) = f$. By function extensionality it suffices to show that
\[ \Pi_{z: \Sigma_{x:A}B(x)} \ind_\Sigma(\lambda x. \lambda y. f(x,y))(t) = f(t).\] By $\Sigma$-induction it suffices to prove this for pairs in which case req require identifications
\[ \ind_\Sigma(\lambda x. \lambda y. f(x,y)(a,b) = f(a,b),\]
but this holds definitionally by the computation rule for $\Sigma$ types.
\end{proof}

In the non-dependent case we have as a corollary:

\begin{cor} For types $A$ and $B$ and $C$,
\[ \term{ev-pair} : (A \times B \to C) \to (A \to B \to C)\]
given by $f \mapsto \lambda a. \lambda b. f(a,b)$ is an equivalence.
\end{cor}

\subsection*{The universal property of identity types}

The universal property for identity types can be understood as an (undirected) type theoretic version of the Yoneda lemma. In the most familiar case, when $B$ is a type family over $A$, it says that the map
\[ \term{ev-refl} : \left( \Pi_{x:A}(a=x) \to B(x) \right) \to B(a)\]
given by $f \mapsto f(a,\refl_a)$ is an equivalence. As before, though, it generalizes to a dependent version of the undirected Yoneda lemma, where the type family $B$ is allowed to depend on $x :A$ and $p: a =x$.

\begin{thm} Consider a type $A$, a term $a :A$, and a type family $B(x,p)$ over $x:A$ and $p: a=x$. Then the map
\[ \term{ev-refl} : \left( \Pi_{x:A}\Pi_{p:a=x} B(x,p) \right) \to B(a,\refl_a)\]
defined by $f \mapsto f(a,\refl_a)$ is an equivalence.
\end{thm}
\begin{proof}
The inverse map is
\[ \pathind_a : B(a,\refl_a) \to \left( \Pi_{x:A}\Pi_{p:a=x} B(x,p) \right),\]
which is a section by the computation rule of the path induction principle.

For the other homotopy $\pathind_a \circ \term{ev-refl} \simeq \id$ let $f : \Pi_{x:A} \Pi_{p:a=x}B(x,p)$. To prove that 
$\pathind_a(f(a,\refl_a)) = f$ we apply function extensionality twice so that it suffices to show that
\[ \Pi_{x:A} \Pi_{p:a=x} \pathind_a(f(a,\refl_a),x,p) = f(x,p).\] This follows from path induction on $p$ since $\pathind_a(f(a,\refl_a),a,\refl_a) \doteq f(a,\refl_a)$ by the computation rule for path induction.
\end{proof}

\subsection*{Composing with equivalences}

Another useful consequence is the fact that $f : A \to B$ is an equivalence if and only if precomposing with $f$ is an equivalence.

\begin{thm} For any map $f \colon A \to B$ the following are logically equivalent:
\begin{enumerate}
\item $f$ is an equivalence
\item For any type family $P$ over $B$ the map
\[ \left( \Pi_{b:B} P(b) \right) \to \left( \Pi_{a:A} P(f(a))\right)\]
given by $h \mapsto h \circ f$ is an equivalence.
\item For any type $C$ the map
\[ (B \to C) \to (A \to C)\]
given by $g \mapsto g \circ f$ is an equivalence.
\end{enumerate}
\end{thm}
\begin{proof}
(ii) immediately implies (iii) by choosing a constant family. 

Assuming (iii) we can take $C=A$ and use the fact that the fibers of the equivalence
\[ - \circ f : (B \to A) \to (A \to A)\]
are contractible to find a point $(h, H) : \fib_{-\circ f}(\id_A) \doteq \Sigma_{h:B \to A} h \circ f = \id_A$.
To see that $h$ is also a section of $f$ we choose $C=B$ and use the fiber of the equivalence
\[ -\circ f : (B \to B) \to (A \to B)\]
over $f$. We have $(\id_B,\refl_f)$ in this fiber but also the point $(f \circ h, fH)$ where $fH$ is the name for the identification derived from the whiskered homotopy $f \cdot H : f \circ h \circ f \sim f$. Since the ifber is contractible, we must have an identification $f \circ h = \id_B$ as desired.

Thus, it remains to prove that (i) implies (ii) which is the hard part. The first step is to promote the equivalence $f$ to a coherently invertible equivalence, involving $g : B \to A$, homopies $G$ and $H$, and a higher homotopy $K : G \cdot f \sim f \cdot H$. We leave the details to \cite[13.4.1]{Rijke}.
\end{proof}

\subsection*{The strong induction principle of \texorpdfstring{$\bN$}{the natural numbers}}

A final application of function extensionality is to prove the strong induction principle for the natural numbers. We give the statement and leave the proof to \cite[\S13.5]{Rijke}. Function extensionality is needed to give the computation rules.

\begin{thm} Consider a type family $P$ over $\bN$ with $p_0 : P(0)$ and
\[ p_S : \Pi_{n: \bN} \left( \Pi_{m: \bN}(m \leq n) \to P(m)\right) \to P(n+1).\]
Then there is a dependent function
\[ \term{strong-ind}_\bN(p_0,p_S) : \Pi_{n : \bN}P(n) \]
so that $\term{strong-ind}_\bN(p_0,p_S,0)  = 0$ and 
\[ \term{strong-ind}_\bN(p_0,p_S, n+1) = p_S(n, \lambda m.\lambda p.\term{strong-ind}_\bN(p_0,p_s,m)).\]
\end{thm}



\section*{October 25: Propositional truncation}

There is a distinction made in mathematics between \emph{properties} and extra \emph{structure}. For instance, we can ask whether a given function $f \colon A \to B$ is surjective or not. In type theory, this is the case just when 
\[ \Pi_{b:B} \Sigma_{a:A} f(a)=b.\]
But we've seen that there is an equivalence of types 
\[ \left( \Pi_{b:B} \Sigma_{a:A} f(a)=b \right) \simeq \left( \Sigma_{s:A \to B} \Pi_{b:B} f(s(b))=b \right).\]
In particular, the data provided by a term in this type provides an explicit section $s : B \to A$ which is additional structure.

To correctly capture the mere property of being surjective, we need a way to assert the \emph{proposition} that a type is inhabited without providing the \emph{data} of a specific inhabitant. The proposition that a type $A$ is inhabited is called the \textbf{propositional truncation} of $A$.

\subsection*{The universal property of propositional truncations}

The propositional truncation of a type $A$ is a type $\mere{A}$ equipped with a map $\eta \colon A \to \mere{A}$ with a universal property to be described. The map ensures that if $a :A$ then the proposition $\mere{A}$ that $A$ is inhabited holds.

\begin{defn} Let $A$ be a type and let $f \colon A \to P$ be a map whose codomain is a proposition. We say that $f$ is a \textbf{propositional truncation} of $A$ if for every proposition $Q$ the map
\[ - \circ f : (P \to Q) \to (A \to Q)\]
is an equivalence.
\end{defn}

This definition describes the \textbf{universal property of the propositional truncation}. It can be reformulated as follows. Note the fiber of the map 
\[ - \circ f : (P \to Q) \to (A \to Q)\]
over $g : A \to Q$ is the type
\[ \sum_{h : P \to Q} h \circ f = g.\]
Thus, $f$ satisfies the universal property of the propositional truncation if and only if these fibers are contractible, meaning that for each map $g : A \to Q$ into a proposition there is a unique map $h : P \to Q$ for wich $h \circ f = g$, a circumstance we might summarize by saying that every map $g : A \to Q$ into a proposition \textbf{extends uniquely along} $f$ as indicated
\[
\begin{tikzcd} A \arrow[dr, "g"] \arrow[d, "f"'] \\ P \arrow[r, dashed, "\exists !"'] & Q
\end{tikzcd}
\]

\begin{rmk} By (weak) function extensionality, the types $(P \to Q)$ and $(A \to Q)$ are propositions, since $Q$ is a proposition. Recall that equivalences between propositions are just logical equivalences. Thus, to prove that \[ - \circ f : (P \to Q) \to (A \to Q)\] is an equivalence, it suffices to construct a function 
\[  (A \to Q) \to (P \to Q)\]
for every proposition $Q$.
\end{rmk}

\begin{prop} Let $A$ be a type and consider two maps $f\colon A \to P$ and $f' \colon A \to P'$ into propositions. If any two of the following hold so does the third:
\begin{enumerate}
\item $f$ is a propositional truncation
\item $f'$ is a propositional truncation
\item There is a (unique) equivalence $P \simeq P'$, commuting with the maps from $A$.
\end{enumerate}
\end{prop}
\begin{proof}
Given (i) and (ii) the universal properties induce maps $P \to P'$ and $P' \to P$ under $A$ that are necessarily an inverse equivalence. This proves (iii). 

Given (iii), we have an equivalence between $(P \to Q)$ and $(P' \to Q)$ and moreover the equivalence $P \leftrightarrow P'$ commutes with the maps from $A$, since by function extensionality any two terms in the types $A \to P$ and $A \to P'$ must be identifiable. But now by the 2-of-3 property for equivalences, if either of the two dashed maps below is an equivalence both are. 
\[
\begin{tikzcd}  (P \to Q) \arrow[rr, leftrightarrow, "\simeq"] \arrow[dr, dashed, "- \circ f"']& & (P' \to Q) \arrow[dl, dashed, "- \circ f'"] \\ & (A \to Q) 
\end{tikzcd}
\] 
Thus when (iii) holds (i) holds if and only if (ii) holds.
\end{proof}

It is tempting to think that a type is inhabited if and only if it is non-empty. That it, it is tempting to wonder whether the canonical map $\lambda x \to \ev_x : A \to \neg\neg A$ is a propositional truncation, since after all $\neg\neg A$ is always a proposition. One can verify that any map $A \to \neg\neg Q$ extends to a map $\neg\neg A \to \neg\neg Q$ proving that the map
\[ (\neg\neg A \to \neg\neg Q) \to (A \to \neg\neg Q)\]
is an equivalence. However, this universal property with respect to doubly negated propositions cannot be extended to general propositions. Indeed, propositional truncations are not guaranteed to exist in Martin L\"{o}f's dependent type theory; see ``Notions of anonymous existence in Martin-L\"{o}f type theory'' by Altenkirch, Coquand, Escard\'{o}, and Kraus for a discussion. However, if we add a new rule to the type theory, we can guarantee their existence.

\subsection*{Propositional truncations as higher inductive types}

The propositional truncation of a general type $A$ can be constructed as an instance of something called a \textbf{higher inductive type}. Higher inductive types are similar to ordinary inductive types but have an additional feature that constructors can be used to generate identifications. As in ordinary inductive types, there are \textbf{point constructors} which introduce new terms, but now these are supplemented by \textbf{path constructors} which introduce new identifications.

In the case of the propositional truncation, for any type $A$, $\mere{A}$ is the type given by one point constructor $\eta \colon A \to \mere{A}$ and one path constructor $\alpha \colon \Pi_{x,y : \mere{A}} x= y$. It follows immediately from the existence of the term $\alpha$ that $\mere{A}$ is a proposition.

The induction principle for the propositional truncation tells us how to construct terms $h : \Pi_{t : \mere{A}} Q(t)$ for any family of types (not only families of propositions). It says that for any family of types $Q$ over $\mere{A}$ if we have
\[ f : \Pi_{a:A}Q(\eta(a))\] and if we can construct identifications $\tr_Q(\alpha(x,y),u)=v$ for all $x,y : \mere{A}$ and all $u : Q(x)$ and $v : Q(y)$ then we obtain a dependent function $h : \Pi_{t : \mere{A}}Q(t)$ equipped with a homotopy $h \circ \eta \sim f$. This homotopy should be thought of as a homotopical version of the computation rule for ordinary inductive types.

\begin{rmk} Although the induction principle for propositional truncations does not a priori require that the type family is a family of propositions, once the second required term
\[ \beta : \Pi_{x,y : \mere{A}} \Pi_{u : Q(x)} \Pi_{v : Q(y)} \tr_Q(\alpha(x,y),u)=v\] exists it follows that $Q$ is a family of propositions. Recall that transporting along a path defines an equivalence and in particular an embedding. Thus
\[ (\tr_Q(\alpha(x,y),u) = \tr_Q(\alpha(x,y),w)) \simeq (u=w)\] for any $u,w : Q(x)$. Taking $v  =  \tr_Q(\alpha(x,y),w))$, we see that $\beta$ defines a term in the left-hand type so it follows that any $u,w : Q(x)$ are identifiable. Thus $Q(x)$ must be a proposition.
\end{rmk}

Since the induction principle of the propositional truncation is only applicable in families of propositions there are no interesting computation rules: there was no need to mention the homotopy $h \circ \eta \sim f$. After all any identification in a proposition just holds!


\begin{thm} The map $\eta \colon A \to \mere{A}$ satisfies the universal property of propositional truncation.
\end{thm}
\begin{proof}
It suffices to construct a map
\[ (A \to Q) \to (\mere{A} \to Q)\]
for any proposition $Q$, which we do by the induction principle of propositional truncation. To construct a term in $\mere{A} \to Q$ we need a term $f : A \to Q$ and also have to prove that $\tr_{Q}(\alpha(x,y), u)=v$ holds for any $u, v :Q$ and any $x,y : \mere{A}$. But $Q$ is a proposition so this is automatic. Thus the map that sends $f : A \to Q$ to the induced function $\mere{A} \to Q$ defines the required function $(A \to Q) \to (\mere{A} \to Q)$.
\end{proof}

Next we show that $\mere{-}$ acts functorially on functions.

\begin{prop} For any pair of types $A$ and $B$ there is a map
\[ \mere{-} \colon (A \to B) \to (\mere{A} \to \mere{B})\]
satisfying $\mere{\id} \sim \id$ and $\mere{g \circ f} \sim \mere{g} \circ \mere{f}$.
\end{prop}
\begin{proof}
For any $f : A \to B$, $\mere{f}$ may be defined to be the unique extension
\[
\begin{tikzcd} A \arrow[r, "f"] \arrow[d, "\eta"'] & B \arrow[d, "\eta"] \\ \mere{A} \arrow[r, dashed, "\mere{f}"'] & \mere{B}
\end{tikzcd}
\]
Note by definition that $\id_{\mere{A}}$ and $\mere{g}\circ\mere{f}$ are similarly extensions of $\id_A$ and $g \circ f$ along $\eta$, and thus must agree up to homotopy with $\mere{\id_A}$ and $\mere{g \circ f}$ by uniqueness.
\end{proof}

\subsection*{Logic in type theory}

Propositional truncations can be used to extend the interpretation of logic in type theory via the Curry-Howard correspondence. This refines our previous discussion by replacing structures with properties.

\begin{defn} Given two propositions $P$ and $Q$ we define their \textbf{disjunction} to be
\[ P \vee Q \coloneq \mere{P + Q}.\]
\end{defn}

\begin{defn} Given a family of propositions $P$ over a type $A$, define
\[ \exists_{x :A} P(x) \coloneq \mere{\Sigma_{x:A}P(x)}.\]
\end{defn}

One can verify the expected logical equivalences involving these notions. For instance:

\begin{prop} For any family of propositions $P$ over $A$ there is a dependent function
\[ \epsilon : \Pi_{a:A} \left( P(a) \to \exists_{x:A}P(x) \right).\]
Furthermore, for any proposition $Q$ we have
\[ \left( (\exists_{x:A}P(x)) \to Q \right) \leftrightarrow \left( \Pi_{x:A}P(x) \to Q \right).\]
\end{prop}
\begin{proof}
Define $\epsilon(a,p) \coloneq \eta(a,p)$. Now consider the following composition of maps
\[ \left( (\exists_{x:A}P(x)) \to Q \right) \to \left( (\Sigma_{x:A} P(x)) \to Q\right) \to \left( \Pi_{x:A}P(x) \to Q \right).\]
The first map is an equivalence by the uinversal property of the propositional truncation while the second is an equivalence by the universal property of $\Sigma$-types.
\end{proof}


\subsection*{Mapping from propositional truncations into sets}

In general it is tricky to map out of propositional truncations into general types but some tricks may help. To define a map $\mere{A} \to X$ one could search for a type family $P$ over $X$ so that $\Sigma_{x:X}P(x)$ is a proposition. Then the universal property of propositional truncation can be used to define a map $\mere{A} \to \Sigma_{x:X}P(x)$ and compositing with $\pr_1$ then defines  map to $X$.

When $X$ is a set there is another strategy. The propositional truncation $\mere{A}$ can be thought of as a quotient of the type $A$ by the equivalence relation that identifies any two terms in $A$ together. We will prove that to extend a map $f : A \to X$ into a set to a map $\mere{A} \to X$ it suffices to define identifications $f(x) =f(y)$ for all $x,y : A$.

\begin{defn} A map $f \colon A \to B$ is \textbf{weakly constant} if it comes with a term in the type
\[ \Pi_{x,y:A} f(x) = f(y).\]
\end{defn}

If $B$ has a term $b$, then weakly constant maps are homotopic to constant maps $\term{const}_b$ but in general there is no requirement for $B$ to have any terms.

\begin{lem} Consider a commutative triangle where $B$ is an arbitrary type
\[
\begin{tikzcd} A \arrow[dr, "f"] \arrow[d, "\eta"'] \\ \mere{A} \arrow[r, "g"'] & B
\end{tikzcd}
\]
then $g$ is weakly constant.
\end{lem}

Kraus observed that any weakly constant map $f : A \to B$ into a set $B$ extends uniquely to a map $\mere{A} \to B$. Thus to define a map $\mere{A} \to B$ into a set it suffices to define $f : A \to B$ and show that it is weakly constant. 

\begin{thm}[Kraus]  For any type $A$ and set $B$ the map
\[  (\mere{A} \to B) \to \Sigma_{f:A \to B} \Pi_{x,y:A} f(x) = f(y)\]
given by $g \mapsto (g \circ \eta, \lambda x .\lambda y. \ap(\alpha(x,y)))$ is an equivalence.
\end{thm}

For proof see \cite[\S 14.4]{Rijke}.

\section*{October 27: The image of a map}

Propositional truncation can be used to define the \textbf{image} of a map $f \colon A \to X$, which can be thought of as the smallest subtype of $X$ that contains all of the values of $f$. More precisely, we'll define a commutative triangle
\[
\begin{tikzcd} A \arrow[rr, "f"] \arrow[dr, "q"'] & & X \\ & \im{f} \arrow[ur, "i", hook]
\end{tikzcd}
\]
in which $i$ is an embedding.

\subsection*{The image of a map}

\begin{defn} Let $f \colon A \to X$ and $g  \colon B \to X$ be maps. A \textbf{morphism} from $f$ to $g$ over $X$ is a map $h \colon A \to B$ together with a homotopy $H \colon f \sim g \circ h$ witnessing commutativity of the following triangle
\[
\begin{tikzcd} A \arrow[rr, "h"] \arrow[dr, "f"'] & & B \arrow[dl, "g"] \\ & X
\end{tikzcd}
\]
Thus we define the type
\[ \hom_X(f,g) \coloneq \Sigma_{h:A \to B} f \sim g \circ h.\]
Composition of morphisms over $X$ is defined by
\[ (k,K) \circ (h,H) \coloneq (k \circ h, H \cdot (K \cdot h)).\]
\end{defn}


\begin{lem} For any $f \colon A \to X$ and any $m : B \to X$ the type, $\hom_X(f,m)$ is a proposition.
\end{lem} 
\begin{proof}
We have an equivalence
\[ \hom_X(f,m) \coloneq  \Sigma_{h:A \to B} f \sim m \circ h \coloneq \Sigma_{h:A \to B} \Pi_{a :A}  f(a) = m(h(a)) \simeq \Pi_{a:A} \Sigma_{b :B} f(a) = m(b) \simeq  \Pi_{a:A} \fib_m(f(a)).\]
Since $m$ is an embedding its fibers are propositions. By weak function extensionality, it follows that $\hom_X(f,m)$ is a proposition.
\end{proof}


\begin{prop}
Consider a commutative triangle 
\[
\begin{tikzcd} A \arrow[rr, "q"] \arrow[dr, "f"'] & & I \arrow[dl, "i"] \\ & X
\end{tikzcd}
\]
with $H : f \sim i \circ q$. 
We say that $i$ satisfies the \textbf{universal property of the image of} $f$ if either of the following logically equivalent conditions hold:
\begin{enumerate}
\item The precomposition function 
\[ - \circ (q,H) : \hom_X(i,m) \to \hom_X(f,m)\]
is an equivalence for any embedding $m \colon B \hookrightarrow X$.
\item For every embedding $m : B \to X$ there is a map
\[ \hom_X(f,m) \to \hom_X(i,m).\]
\end{enumerate}
\end{prop}
\begin{proof}
Since $\hom_X(i,m)$ and $\hom_X(f,m)$ are propositions any map between them is an equivalence if and only if there is exists any map in the other direction.
\end{proof}

Propositional truncation can be used to construct the image of a map:

\begin{defn} For any map $f \colon A \to X$ we define the \textbf{image} of $f$ to be the type
\[ \im{f} \coloneq \Sigma_{x:X} \mere{\fib_f(x)}\]
The \textbf{image inclusion} $i_f \colon \im{f} \to X$ is the first projection $\pr_1$. The map $q_f \colon A \to \im{f}$ is given by
\[ q_f(a) \coloneq (f(a), \eta(a,\refl_{f(a)})).\] The homotopy $I_f : f \sim i_f \circ q_f$ is given by $I_f(a) \coloneq \refl_{f(a)}$.
\end{defn}

We now verify that this construction has the required properties.

\begin{lem} The image inclusion $i_f \colon \im{f} \to X$ is an embedding.
\end{lem}
\begin{proof}
The fiber of $\pr_1 \colon \Sigma_{x:X} \mere{\fib_f(x)} \to X$ is equivalent to $\mere{\fib_f(x)}$, which is a proposition. Thus $i_f \coloneq \pr_1$ is an embedding.
\end{proof}

Theorem \cite[15.1.7]{Rijke} verifies the following:

\begin{thm} For any $f \colon A \to X$ the image inclusion $i_f \colon \im{f} \to X$ satisfies the universal property.
\end{thm}

\subsection*{Surjective maps}

As mentioned above, our previous notion of surjectivity of a map $f \colon A \to B$ is more properly ``split surjectivity'' as 
\[ \left( \Pi_{b:B} \Sigma_{a:A} f(a) =b \right) \simeq \left( \Sigma_{s : A \to B} \Pi_{a:A} f(s(a)) = b\right) \eqcolon  \left( \Sigma_{s: A \to B} f \circ s \sim \id_B \right).\]
The traditional notion is more closely analogous to:

\begin{defn} A map $f \colon A \to B$ is \textbf{surjective} if there is a term in the type
\[ \is{surj}(f) \coloneq \Pi_{b:B} \mere{\fib_f(b)}.\]
\end{defn}

Note that having a section is stronger than surjectivity because we have a map $\fib_f(b) \to \mere{\fib_f(b)}$ but don't typically have a map in the reverse direction.

Surjective maps also have a universal property recorded by the following theorem. See \cite{Rijke} for a proof:

\begin{prop} For a map $f \colon A \to B$, the following are logically equivalent:
\begin{enumerate}
\item $f \colon A \to B$ is surjective.
\item For any family $P$ of propositions over $B$, 
\[ -\circ f : \left( \Pi_{b:B} P(b) \right) \to \left( \Pi_{a:A} P(f(a)) \right)\]
is an equivalence.
\end{enumerate}
\end{prop}
%\begin{proof}
%Suppose that $f$ is surjective and consider the square
%\[
%\begin{tikzcd}
%\Pi_{b:B} P(b) \arrow[r, "-\circ f"] \arrow[d, "{h \mapsto \lambda b. \term{const}_{h(b)}}"'] & \Pi_{a:A} P(f(a)) \\\Pi_{b:B} \mere{\fib_f(b)} \to P(b) \arrow[r, "h \mapsto h(-)\circ \eta"'] & \Pi_{b:B} \fib_f(b) \to P(b) \arrow[u, "{h \mapsto \lambda a . h(f(a), (a, \refl))}"']
%\end{tikzcd}
%\]
%\end{proof}


It follows that:

\begin{cor} For any $f \colon A \to P$ into a proposition, the following are logically equivalent:
\begin{enumerate}
\item $f \colon A \to P$ is a propositional truncation of $A$.
\item $f \colon A \to P$ is surjective.
\end{enumerate}
\end{cor}
\begin{proof}
The second equivalent condition above is the universal property of propositional truncation, except without the hypothesis that $B$ itself is a proposition.
\end{proof}

We now see that the map $q$ in the image factorization is surjective:

\begin{thm} Consider a commutative triangle
\[
\begin{tikzcd} A \arrow[rr, "f"] \arrow[dr, "q"'] & & X \\ & B \arrow[ur, "m"']
\end{tikzcd}
\]
in which $m$ is an embedding. Then $m$ is the image inclusion of $f$ if and only if $q$ is surjective.
\end{thm}
\begin{proof}
First suppose $m$ is the image inclusion of $f$. In that case the function
\[
\begin{tikzcd} \left( \Sigma_{b:B} \mere{\fib_q(b)} \right) \arrow[r, "\pr_1"] & B \arrow[r, "m"] & X
\end{tikzcd}
\]
is an embedding as a composite of embeddings. By the universal property of $m$ there is a unique map 
\[
\begin{tikzcd} B \arrow[dr, "m"'] \arrow[rr, dashed, "h"] & & \left( \Sigma_{b:B} \mere{\fib_q(b)} \right)  \arrow[dl, "m \circ \pr_1"]
\\ & X
\end{tikzcd}
\]
making the triangle commute. Thus $m \circ (\pr_1 \circ h) \sim m$ but also $m \circ \id  \sim m$. By the uniqueness in the universal property of $m$, it follows that $\pr_1 \circ h \sim\id$. Thus $h$ is a section of the projection map. In particular, this defines a dependent function in
\[  \Pi_{b:B} \mere{\fib_q(b)}\]
proving that $q$ is surjective.

Conversely, if $q$ is surjective, we can prove that $m$ is the image of $f$ by constructing an equivalence
\[ \hom_X(f,m') \to \hom_x(m,m')\]
for any embedding $m' \colon B' \to X$. Using the equivalence noted above, we calculate
\[ \hom_X(m,m') \simeq \Pi_{b:B} \fib_{m'}(m(b)) \simeq \Pi_{a:A} \fib_{m'}(m(q(a))) \simeq \Pi_{a:A} \fib_{m'}(f(a)) \simeq \hom_X(f,m'),\] where the second equivalence uses the hypothesis that $q$ is surjective and the third equivalence follows from the homotopy $f \sim m \circ q$.
\end{proof}

\begin{cor} Every map factors uniquely as a surjective map followed by an embedding.
\end{cor}
\begin{proof}
If $f \colon A \to X$ admits two such factorizations $I : f \sim  i\circ q$ and $I': f \sim i' \circ q'$ then both embeddings $i$ and $i'$ have the universal property of the image of $f$. It follows that there is an equivalence $(e,H) : \hom_X(i,i')$ and moreover $(e,H) \circ (q,I) = (q',I')$
\end{proof}

\section*{Cantor's diagonalization argument}

\begin{defn} For any type $X$ and universe $\UU$ define the $\UU$-power set of $X$ to be 
\[ P_{\UU}(X) \coloneq X \to \type{Prop}_{\UU}\]
using the universe of propositions in $\UU$.
\end{defn}

\begin{thm} For any type $X$ and universe $\UU$ there is no surjective function $f \colon X \to P_{\UU}(X)$.
\end{thm}
\begin{proof}
We're asked to prove a negation so we may consider a function $f \colon X \to (X \to \type{Prop}_{\UU})$ and suppose that $f$ is surjective. Following Cantor's diagonalization argument, define the subset $P \colon X \to \type{Prop}_{\UU}$ by
\[ P(x) \coloneq \neg(f(x,x)).\]
Since $f$ is assumed to be surjective and our goal is to reach a contradiction, it suffices to show that
\[ \mere{\Sigma_{x:X} f(x)=P} \to \emptyset.\]
Since $\emptyset$ is a proposition, by the universal property of propositional truncation it is equivalent to show that
\[ \left( {\Sigma_{x:X} f(x)=P}  \right) \to \emptyset.\]
So we consider $x:X$ and an identification $f(x)=P$. From the identification it follows that the propositions $f(x,y)$ and $P(y)$ are logically equivalent for all $y : Y$. In particular $f(x,x)$ is logically equivalent to $P(x)$ but since $P(x) \coloneq \neg f(x,x)$ we have a logical equivalence between $f(x,x)$ and $\neg(f(x,x))$. This gives our desired contradiction.
\end{proof}

\section*{November 1: The univalence axiom}
\section*{November 3: Set quotients}
\section*{November 8: Groups}
\section*{November 10: Algebra}
\section*{November 15: The real numbers}


\part{Synthetic Homotopy Theory}

\section*{November 17: The circle}
\section*{November 29: The universal cover of the circle}
\section*{December 1: Homotopy groups of types}
\section*{December 6: Classifying types of groups}

\bibliographystyle{alpha}
\begin{thebibliography}{R}

\bibitem[R]{Rijke} Egbert Rijke, \emph{Introduction to Homotopy Type Theory}, available from \url{https://hott.zulipchat.com}

\end{thebibliography}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-master: t
%%% End:
